# ðŸ” Fake News Detector

**CDS525 Group Project** â€” Deep Learning-based Fake News Detection

Two model architectures with comprehensive experiments, visualizations, and explainability.

| Model | Test Accuracy | Parameters | Training Time (T4 GPU) |
|-------|:------------:|:----------:|:---------------------:|
| **BiLSTM + Attention + GloVe** | **97.35%** | ~0.5M trainable | ~3 min/experiment |
| **DistilBERT (Transformer)** | **~99%** | 66M (fine-tuned) | ~8 min/experiment |

---

## ðŸ“ Project Structure

```
fakenews-detector/
â”œâ”€â”€ src/                          # Core modules (BiLSTM project)
â”‚   â”œâ”€â”€ data_utils.py             # Data loading, cleaning, vocabulary, GloVe
â”‚   â”œâ”€â”€ data_augment.py           # External dataset loading & EDA augmentation
â”‚   â”œâ”€â”€ model.py                  # BiLSTM + Attention classifier
â”‚   â”œâ”€â”€ trainer.py                # Training loop, FocalLoss, early stopping
â”‚   â”œâ”€â”€ visualize.py              # All 8 required figures
â”‚   â””â”€â”€ chain_of_thought.py       # Attention-based explainability (CoT)
â”œâ”€â”€ main.py                       # Local entry point (runs all experiments)
â”œâ”€â”€ FakeNewsDetection_Colab.ipynb  # â˜… BiLSTM â€” Google Colab notebook
â”œâ”€â”€ BERT_FakeNews_Colab.ipynb      # â˜… DistilBERT â€” Google Colab notebook
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ README.md
```

---

## ðŸ“¦ Dataset

The project uses **3 datasets** merged together (~50K total samples):

### 1. Main Dataset â€” `fakenews 2.csv` (4,987 rows)

- **Format**: `text, label` (0=Fake, 1=Real)
- **Source**: Course-provided dataset
- **Download**: Available from the course materials, or use a similar fake news dataset from [Kaggle](https://www.kaggle.com/)

### 2. Extra Dataset â€” `News _dataset/` (44,898 rows)

This is the **Kaggle Fake and Real News Dataset** by ClÃ©ment Bisaillon.

| File | Rows | Label |
|------|:----:|:-----:|
| `Fake.csv` | 23,481 | 0 (Fake) |
| `True.csv` | 21,417 | 1 (Real) |

- **Format**: `title, text, subject, date`
- **Download from Kaggle**: [https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset)

#### Download Steps:
1. Go to the Kaggle link above â†’ Click **Download** (éœ€è¦ Kaggle è´¦å·)
2. Unzip to get `Fake.csv` and `True.csv`
3. Place them in a folder called `News _dataset/` (note the space before underscore)

### 3. GloVe Pre-trained Embeddings (BiLSTM only)

- **File**: `glove.6B.100d.txt` (347 MB)
- **Download**: [https://nlp.stanford.edu/data/glove.6B.zip](https://nlp.stanford.edu/data/glove.6B.zip) (822 MB zip)
- Extract `glove.6B.100d.txt` from the zip file
- The Colab notebook **downloads this automatically** â€” no manual action needed

### Final Data Directory Layout

```
project_root/
â”œâ”€â”€ fakenews 2.csv
â”œâ”€â”€ News _dataset/
â”‚   â”œâ”€â”€ Fake.csv
â”‚   â””â”€â”€ True.csv
â””â”€â”€ glove.6B.100d.txt          # BiLSTM only; auto-downloaded on Colab
```

After merging & deduplication: **43,458 unique samples** (20,733 Fake + 22,720 Real)

With data augmentation (EDA): **60,832 training samples**

---

## ðŸš€ How to Run

### Option A: Google Colab (Recommended)

> **No local setup needed.** Just upload data to Google Drive and run.

#### BiLSTM + GloVe:
1. Upload `fakenews 2.csv` and `News _dataset/` folder to **Google Drive** â†’ `My Drive/fakenews/`
2. Open `FakeNewsDetection_Colab.ipynb` in Colab
3. Set Runtime â†’ **GPU (T4)**
4. Run all cells (GloVe is auto-downloaded)
5. ~30 min total for all experiments

#### DistilBERT:
1. Same data setup as above
2. Open `BERT_FakeNews_Colab.ipynb` in Colab
3. Set Runtime â†’ **GPU (T4)** (A100 recommended for faster training)
4. Run all cells
5. ~60 min total for all experiments

### Option B: Local Machine

```bash
# 1. Clone the repository
git clone https://github.com/ccccsuper0828/fakenews-detector.git
cd fakenews-detector

# 2. Install dependencies
pip install -r requirements.txt

# 3. Place data files (see Dataset section above)
#    - fakenews 2.csv
#    - News _dataset/Fake.csv, News _dataset/True.csv
#    - glove.6B.100d.txt

# 4. Run all experiments
python main.py
```

**Requirements**: Python 3.8+, PyTorch 2.0+, CUDA GPU recommended (CPU works but ~10x slower)

---

## ðŸ—ï¸ Model Architectures

### Model 1: BiLSTM + Attention + GloVe

```
Input Text â†’ Tokenize â†’ GloVe Embedding (100d, frozen)
  â†’ BiLSTM (2 layers, 128 hidden, bidirectional)
  â†’ Attention Mechanism (2-layer MLP)
  â†’ Weighted Context Vector (256d)
  â†’ FC(256â†’64) â†’ ReLU â†’ Dropout â†’ FC(64â†’1) â†’ Sigmoid
```

**Key Features**:
- GloVe pre-trained embeddings (98.5% vocabulary coverage)
- Frozen embeddings â†’ reduces 2M trainable params, prevents overfitting
- Smart truncation: keeps 70% head + 30% tail of long articles
- Stopword removal for cleaner signal

### Model 2: DistilBERT (Transformer)

```
Input Text â†’ BERT Tokenizer (subword, max 256 tokens)
  â†’ DistilBERT (6 layers, 12 attention heads, 768d)
  â†’ [CLS] Token Representation
  â†’ FC(768â†’256) â†’ ReLU â†’ Dropout â†’ FC(256â†’1) â†’ Sigmoid
```

**Key Features**:
- Pre-trained on BookCorpus + Wikipedia (3.3B words)
- Minimal text cleaning (BERT needs punctuation & stopwords for context)
- "Reuters" text removed to prevent data leakage
- Linear warmup scheduler (standard for Transformer fine-tuning)

---

## âš™ï¸ Training Strategy

| Component | BiLSTM | DistilBERT |
|-----------|--------|------------|
| **Optimizer** | AdamW (lr=0.001, wd=1e-4) | AdamW (lr=2e-5, wd=0.01) |
| **LR Scheduler** | ReduceLROnPlateau | Linear Warmup (10%) |
| **Loss Functions** | BCE / Focal Loss | BCE / Focal Loss |
| **Regularization** | Dropout(0.5), Frozen GloVe | Dropout(0.2) |
| **Early Stopping** | Patience=5 on val_acc | Patience=3 on val_acc |
| **Gradient Clipping** | max_norm=1.0 | max_norm=1.0 |
| **Epochs** | 20 (max) | 4 (max) |
| **Data Augmentation** | EDA (random delete/swap) | EDA (random delete/swap) |

---

## ðŸ“Š Experiments & Figures

Both notebooks generate **8 required figures** for the course report:

| Figure | Description |
|--------|-------------|
| **Fig 1** | Training curves â€” BCE Loss (default config) |
| **Fig 2** | Training curves â€” Focal Loss (default config) |
| **Fig 3** | Learning Rate comparison â€” BCE Loss (4 LRs) |
| **Fig 4** | Learning Rate comparison â€” Focal Loss (4 LRs) |
| **Fig 5** | Batch Size comparison â€” BCE Loss (4 sizes) |
| **Fig 6** | Batch Size comparison â€” Focal Loss (4 sizes) |
| **Fig 7** | Test Predictions table (top 100 samples) |
| **Fig 8** | Confusion Matrix |
| **Fig 9** | ROC-AUC Curve (DistilBERT only) |

---

## ðŸ§  Chain-of-Thought (CoT) Explainability

Both models include a **Chain-of-Thought reasoning module** that explains predictions:

```
Step 1 - Text Feature Analysis:
  [Sensational Language] LOW
  [Source Credibility]   HIGH
    Found: according to, officials said
  [Emotional Tone]      LOW
  [Clickbait Pattern]   LOW

Step 2 - Model Attention Key Words:
  'reuters'     [0.0812] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  'officials'   [0.0654] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ
  'government'  [0.0531] â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ

Step 3 - Reasoning Chain:
  1. The text references credible sources, consistent with legitimate news.
  2. The text uses neutral, factual language consistent with professional journalism.
  3. The text maintains an objective tone without excessive emotional manipulation.

Conclusion: This article is classified as [Real] with 99.87% confidence.
```

The CoT module analyzes:
1. **Text features**: sensational words, credibility indicators, emotional tone, clickbait patterns
2. **Model attention weights**: which words/tokens the model focused on most
3. **Reasoning chain**: step-by-step logical explanation combining both signals

---

## ðŸ“ˆ Results Summary (BiLSTM on Colab T4 GPU)

| Config | Val Acc | Test Acc |
|--------|:-------:|:--------:|
| BCE, LR=0.001, BS=32 (default) | 97.03% | 97.23% |
| Focal, LR=0.001, BS=32 | 97.08% | 97.18% |
| BCE, LR=0.001, **BS=32** (best) | **97.22%** | **97.35%** |
| BCE, LR=0.01 (too high) | NaN | Crashed |
| BCE, LR=0.0001 | ~96.5% | ~96.5% |

---

## ðŸ› ï¸ Dependencies

```
torch>=2.0.0
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0
matplotlib>=3.7.0
seaborn>=0.12.0
nltk>=3.8.0
tqdm>=4.65.0
transformers          # DistilBERT notebook only
accelerate            # DistilBERT notebook only
```

---

## ðŸ“š References

- **GloVe**: Pennington et al., "GloVe: Global Vectors for Word Representation", EMNLP 2014
- **EDA**: Jason Wei & Kai Zou, "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks", 2019
- **DistilBERT**: Sanh et al., "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", NeurIPS 2019 Workshop
- **Focal Loss**: Lin et al., "Focal Loss for Dense Object Detection", ICCV 2017
- **Dataset**: ClÃ©ment Bisaillon, "Fake and Real News Dataset", Kaggle

---

## ðŸ“„ License

This project is for academic purposes (CDS525 Course Project).
