# Fake News Detection with Deep Learning

**CDS525 Group Project** — BiLSTM + Attention + GloVe Pre-trained Embeddings

---

## Project Overview

This project implements a **BiLSTM + Attention** model with **GloVe pre-trained word embeddings** for binary fake news classification (Fake vs Real). The model is trained on a merged dataset of ~43K news articles with text data augmentation, achieving **97.23% test accuracy**.

A **DistilBERT** (Transformer-based) notebook is also provided as an alternative approach, but has not yet been evaluated.

---

## Experiment Results (Actual)

All results below are from the Colab experiment run saved in `fakenews——result/`.

### Final Evaluation (BCE + GloVe Default Config)

| Metric | Value |
|--------|:-----:|
| **Test Accuracy** | **97.23%** |
| **Precision** | 98.16% |
| **Recall** | 96.50% |
| **F1 Score** | 97.33% |

### Experiment 1: BCE Loss — Default Config (LR=0.001, BS=32)

- Best Validation Accuracy: **97.03%** (Epoch 5)
- Corresponding Test Accuracy: **97.23%**
- Early stopped at Epoch 10

### Experiment 2: Focal Loss — Default Config (LR=0.001, BS=32)

- Best Validation Accuracy: **97.08%** (Epoch 8)
- Corresponding Test Accuracy: **97.18%**
- Early stopped at Epoch 13

### Experiment 3: Learning Rate Comparison (BCE)

| Learning Rate | Best Val Acc | Epochs | Note |
|:------------:|:-----------:|:------:|------|
| 0.01 | 96.48% | 6 | Diverged (NaN at Epoch 5) |
| **0.001** | **97.01%** | 15 | Best LR |
| 0.0001 | 96.71% | 15 | Slower convergence |
| 1e-05 | 96.20% | 20 | Very slow, still improving |

### Experiment 4: Batch Size Comparison (BCE, LR=0.001)

| Batch Size | Best Val Acc | Best Test Acc | Epochs |
|:----------:|:-----------:|:------------:|:------:|
| 16 | 96.96% | 96.97% | 9 |
| **32** | **97.28%** | **97.43%** | 16 |
| 64 | 96.92% | 96.95% | 19 |
| 128 | 96.89% | 96.87% | 17 |

> **Note**: The highest test accuracy observed across all experiments was **97.43%** (BCE, BS=32, Epoch 11 in BS comparison). The final evaluation used the Experiment 1 default model (97.23%).

---

## Generated Figures

All 8 figures were generated by the Colab notebook:

| Figure | File | Description |
|--------|------|-------------|
| Fig 1 | `fig1_bce_default.png` | Training curves — BCE Loss (default config) |
| Fig 2 | `fig2_focal_default.png` | Training curves — Focal Loss (default config) |
| Fig 3 | `fig3_lr_bce.png` | Learning Rate comparison — BCE Loss (4 LRs) |
| Fig 4 | `fig4_lr_focal.png` | Learning Rate comparison — Focal Loss (4 LRs) |
| Fig 5 | `fig5_bs_bce.png` | Batch Size comparison — BCE Loss (4 sizes) |
| Fig 6 | `fig6_bs_focal.png` | Batch Size comparison — Focal Loss (4 sizes) |
| Fig 7 | `fig7_predictions.png` | Test Predictions table (top 100 samples) |
| Fig 8 | `fig8_confusion_matrix.png` | Confusion Matrix |

Figures 3–8 are saved in the `fakenews——result/` directory. All figures (1–8) are also embedded in the Colab notebook output.

---

## Project Structure

```
fakenews-detector/
├── src/                              # Core modules (BiLSTM project)
│   ├── __init__.py
│   ├── data_utils.py                 # Data loading, cleaning, vocabulary, GloVe
│   ├── data_augment.py               # External dataset loading & EDA augmentation
│   ├── model.py                      # BiLSTM + Attention classifier
│   ├── trainer.py                    # Training loop, FocalLoss, early stopping
│   ├── visualize.py                  # 8 required figures
│   └── chain_of_thought.py           # Attention-based explainability (CoT)
├── main.py                           # Local entry point (runs all experiments)
├── FakeNewsDetection_Colab.ipynb      # ★ BiLSTM — Google Colab notebook (recommended)
├── BERT_FakeNews_Colab.ipynb          # DistilBERT — Google Colab notebook (code only, not yet run)
├── fakenews——result/                  # Actual experiment outputs (notebook + figures)
│   ├── FakeNewsDetection_Colab.ipynb  # Notebook with full training logs & inline figures
│   ├── fig3_lr_bce.png ~ fig8_confusion_matrix.png
│   └── 文字文稿1.docx
├── requirements.txt                   # Python dependencies
├── .gitignore
└── README.md
```

---

## Dataset

The project merges **2 datasets** into one (~43K unique samples after deduplication):

### 1. Main Dataset — `fakenews 2.csv`

- **Rows**: 4,986
- **Format**: `text, label` (0 = Fake, 1 = Real)
- **Source**: Course-provided dataset

### 2. External Dataset — `News _dataset/`

This is the **Kaggle Fake and Real News Dataset** by Clément Bisaillon.

| File | Rows | Label |
|------|:----:|:-----:|
| `Fake.csv` | 23,481 | 0 (Fake) |
| `True.csv` | 21,417 | 1 (Real) |

- **Format**: `title, text, subject, date`
- **Download**: [https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset)

#### How to Download:
1. Visit the Kaggle link above (requires a Kaggle account)
2. Click **Download** to get the zip file
3. Unzip to get `Fake.csv` and `True.csv`
4. Place them in a folder named `News _dataset/` (note the space before underscore)

### 3. GloVe Pre-trained Embeddings (BiLSTM only)

- **File**: `glove.6B.100d.txt` (347 MB)
- **Download**: [https://nlp.stanford.edu/data/glove.6B.zip](https://nlp.stanford.edu/data/glove.6B.zip) (822 MB zip)
- Extract `glove.6B.100d.txt` from the zip
- **The Colab notebook downloads this automatically** — no manual action needed for Colab

### Data Processing Pipeline

```
Raw Data:
  fakenews 2.csv (4,986) + Fake.csv (23,481) + True.csv (21,417) = 49,884 total

After Merge & Deduplication:
  43,458 unique samples (Fake: 20,738, Real: 22,720)

After Text Cleaning:
  43,453 samples (Fake: 20,733, Real: 22,720)

Train/Val/Test Split (70% / 10% / 20%):
  Train: 30,416 | Val: 4,346 | Test: 8,691

After Data Augmentation (EDA, num_aug=1):
  Train: 60,832 (augmented) | Val: 4,346 | Test: 8,691
```

### Data Directory Layout

```
project_root/
├── fakenews 2.csv
├── News _dataset/
│   ├── Fake.csv
│   └── True.csv
└── glove.6B.100d.txt          # BiLSTM only; auto-downloaded on Colab
```

---

## How to Run

### Option A: Google Colab (Recommended)

> No local setup needed. Just upload data to Google Drive and run.

1. Upload `fakenews 2.csv` and `News _dataset/` folder to **Google Drive** → `My Drive/fakenews/`
2. Open `FakeNewsDetection_Colab.ipynb` in Colab
3. Set Runtime → **GPU (T4)**
4. Run all cells sequentially
5. GloVe embeddings are downloaded automatically
6. Total runtime: ~90 minutes on T4 GPU (all experiments)

### Option B: Local Machine

```bash
# 1. Clone the repository
git clone https://github.com/ccccsuper0828/fakenews-detector.git
cd fakenews-detector

# 2. Install dependencies
pip install -r requirements.txt

# 3. Place data files (see Dataset section above)
#    - fakenews 2.csv         (in project root)
#    - News _dataset/Fake.csv (in News _dataset/ folder)
#    - News _dataset/True.csv (in News _dataset/ folder)
#    - glove.6B.100d.txt      (in project root)

# 4. Run all experiments
python main.py
```

**Requirements**: Python 3.8+, PyTorch 2.0+, CUDA GPU recommended

---

## Model Architecture

### BiLSTM + Attention + GloVe

```
Input Text
  → Text Cleaning (lowercase, remove HTML/URLs, remove stopwords)
  → Tokenize + Smart Truncation (70% head + 30% tail, max_length=500)
  → GloVe Embedding (100d, frozen)
  → BiLSTM (2 layers, hidden_dim=128, bidirectional → output 256d)
  → Attention Mechanism (2-layer MLP → attention weights)
  → Weighted Context Vector (256d)
  → FC(256→64) → ReLU → Dropout(0.3) → FC(64→1) → Sigmoid
```

### Default Hyperparameters

| Parameter | Value |
|-----------|:-----:|
| Embedding Dim | 100 (GloVe) |
| Hidden Dim | 128 |
| LSTM Layers | 2 |
| Dropout | 0.5 |
| Max Vocab Size | 20,000 |
| Max Sequence Length | 500 |
| Optimizer | AdamW |
| Learning Rate | 0.001 |
| Weight Decay | 1e-4 |
| LR Scheduler | ReduceLROnPlateau |
| Max Epochs | 20 |
| Batch Size | 32 |
| Early Stopping | Patience = 5 (on val accuracy) |
| Gradient Clipping | max_norm = 1.0 |
| Freeze GloVe | True |
| GloVe Coverage | 98.50% |

### Key Techniques

- **GloVe Pre-trained Embeddings**: 98.50% vocabulary coverage, frozen during training to reduce ~2M trainable parameters
- **Smart Truncation**: Keeps 70% head + 30% tail of long articles to preserve both introduction and conclusion
- **Data Augmentation (EDA)**: Random deletion and random swap to double the training set
- **Stopword Removal**: Removes high-frequency functional words to focus on content words
- **Focal Loss**: Alternative to BCE, focuses on hard-to-classify examples (α=0.25, γ=2.0)
- **ReduceLROnPlateau**: Halves learning rate when validation accuracy plateaus (patience=4, factor=0.5)
- **Early Stopping**: Stops training after 5 epochs without validation accuracy improvement

---

## Chain-of-Thought (CoT) Explainability

The model includes a **Chain-of-Thought reasoning module** that explains predictions step by step:

```
Sample 1:
Text Preview: five republican candidates join bigot pastor says god punishes
              gays aids video...

Prediction: Fake (Confidence: 100.00%)

Step 1 - Text Feature Analysis:
  [Sensational Language] LOW
  [Source Credibility]   LOW
  [Emotional Tone]       LOW (Found: hate)
  [Clickbait Pattern]    MEDIUM (Found: believe)
  [Text Length]           301 words

Step 2 - Model Attention Key Words:
  'alex'     [0.0782] ███████████████████████████████████████
  'proimos'  [0.0781] ███████████████████████████████████████
  'via'      [0.0780] ██████████████████████████████████████
  'flickr'   [0.0778] ██████████████████████████████████████
  'image'    [0.0776] ██████████████████████████████████████

Step 3 - Reasoning Chain:
  1. The text lacks references to credible sources, reducing its reliability.
  2. Clickbait-style phrases are present, indicating engagement over accuracy.

Conclusion: This article is classified as [Fake] with 100.00% confidence.
```

The CoT module combines:
1. **Rule-based text feature analysis** — sensational words, credibility indicators, emotional tone, clickbait patterns
2. **Model attention weights** — which words the BiLSTM attention mechanism focused on
3. **Reasoning chain** — step-by-step logical explanation combining both signals

---

## Dependencies

```
torch>=2.0.0
pandas>=2.0.0
numpy>=1.24.0
scikit-learn>=1.3.0
matplotlib>=3.7.0
seaborn>=0.12.0
nltk>=3.8.0
tqdm>=4.65.0
```

For the DistilBERT notebook (if running):
```
transformers
accelerate
```

---

## References

- **GloVe**: Pennington et al., "GloVe: Global Vectors for Word Representation", EMNLP 2014
- **EDA**: Jason Wei & Kai Zou, "EDA: Easy Data Augmentation Techniques for Boosting Performance on Text Classification Tasks", 2019
- **DistilBERT**: Sanh et al., "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", NeurIPS 2019 Workshop
- **Focal Loss**: Lin et al., "Focal Loss for Dense Object Detection", ICCV 2017
- **Dataset**: Clément Bisaillon, "Fake and Real News Dataset", Kaggle

---

## License

This project is for academic purposes (CDS525 Course Project).
