{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Detection with Deep Learning\n",
    "## CDS525 Group Project\n",
    "\n",
    "**Model**: BiLSTM + Attention + GloVe Pre-trained Embeddings\n",
    "\n",
    "**Features**:\n",
    "- ★ Merged Dataset: Original (5K) + News_dataset (45K) ≈ 50K samples\n",
    "- ★ Text Data Augmentation (EDA: Random Deletion / Random Swap)\n",
    "- ★ GloVe Pre-trained Word Embeddings (97%+ coverage)\n",
    "- ★ Frozen GloVe → Reduces 2M trainable params, prevents overfitting\n",
    "- ★ AdamW + Weight Decay + LR Scheduler + Early Stopping\n",
    "- ★ Chain-of-Thought Reasoning for Explainability\n",
    "\n",
    "**Required**: GPU Runtime (Runtime → Change runtime type → T4/A100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Environment Setup\n",
    "# ============================================================\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"⚠ No GPU detected! Go to Runtime → Change runtime type → GPU\")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload Data Files\n",
    "\n",
    "**Option A (推荐)**: Mount Google Drive → 把数据文件放在 Google Drive 里\n",
    "\n",
    "**Option B**: 直接上传文件\n",
    "\n",
    "需要的文件:\n",
    "1. `fakenews 2.csv` — 主数据集\n",
    "2. `News _dataset/Fake.csv` — 外部假新闻数据集\n",
    "3. `News _dataset/True.csv` — 外部真新闻数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Mount Google Drive & Set Data Paths\n",
    "# ============================================================\n",
    "# Option A: Google Drive (推荐 — 数据持久保存)\n",
    "# 请先在 Google Drive 创建文件夹 \"fakenews\", 上传数据文件:\n",
    "#   My Drive/fakenews/fakenews 2.csv\n",
    "#   My Drive/fakenews/News _dataset/Fake.csv\n",
    "#   My Drive/fakenews/News _dataset/True.csv\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "\n",
    "# ★ 修改这里指向你 Google Drive 中的数据目录\n",
    "DATA_ROOT = '/content/drive/MyDrive/fakenews'\n",
    "\n",
    "DATA_PATH = os.path.join(DATA_ROOT, 'fakenews 2.csv')\n",
    "EXTRA_DATA_DIR = os.path.join(DATA_ROOT, 'News _dataset')\n",
    "OUTPUT_DIR = '/content/outputs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# 验证文件存在\n",
    "for p in [DATA_PATH,\n",
    "          os.path.join(EXTRA_DATA_DIR, 'Fake.csv'),\n",
    "          os.path.join(EXTRA_DATA_DIR, 'True.csv')]:\n",
    "    if os.path.exists(p):\n",
    "        size_mb = os.path.getsize(p) / 1e6\n",
    "        print(f\"  ✓ {os.path.basename(p)} ({size_mb:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  ✗ NOT FOUND: {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3: Download GloVe Pre-trained Embeddings\n",
    "# ============================================================\n",
    "import os, zipfile, urllib.request\n",
    "\n",
    "GLOVE_DIR = '/content/glove'\n",
    "GLOVE_PATH = os.path.join(GLOVE_DIR, 'glove.6B.100d.txt')\n",
    "\n",
    "if not os.path.exists(GLOVE_PATH):\n",
    "    os.makedirs(GLOVE_DIR, exist_ok=True)\n",
    "    glove_url = 'https://nlp.stanford.edu/data/glove.6B.zip'\n",
    "    zip_path = os.path.join(GLOVE_DIR, 'glove.6B.zip')\n",
    "\n",
    "    print(\"Downloading GloVe embeddings (822 MB)... This may take a few minutes.\")\n",
    "    urllib.request.urlretrieve(glove_url, zip_path)\n",
    "    print(\"Download complete. Extracting...\")\n",
    "\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extract('glove.6B.100d.txt', GLOVE_DIR)\n",
    "\n",
    "    os.remove(zip_path)  # 删除zip节省空间\n",
    "    print(f\"✓ GloVe extracted: {GLOVE_PATH}\")\n",
    "else:\n",
    "    print(f\"✓ GloVe already exists: {GLOVE_PATH}\")\n",
    "\n",
    "print(f\"  File size: {os.path.getsize(GLOVE_PATH) / 1e6:.0f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define All Modules\n",
    "\n",
    "Below cells define the complete model pipeline (inline, no external files needed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 4: Data Augmentation Module (data_augment)\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "数据增强模块: 加载外部数据集 + EDA文本增强\n",
    "\"\"\"\n",
    "import os, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ========================= 加载外部新闻数据集 =========================\n",
    "def load_news_dataset(dataset_dir):\n",
    "    \"\"\"加载 News_dataset 目录下的 Fake.csv 和 True.csv\"\"\"\n",
    "    fake_path = os.path.join(dataset_dir, \"Fake.csv\")\n",
    "    true_path = os.path.join(dataset_dir, \"True.csv\")\n",
    "    dfs = []\n",
    "\n",
    "    if os.path.exists(fake_path):\n",
    "        df_fake = pd.read_csv(fake_path)\n",
    "        print(f\"    外部数据集 Fake.csv: {len(df_fake)} 条\")\n",
    "        if 'title' in df_fake.columns:\n",
    "            df_fake['text'] = df_fake.apply(\n",
    "                lambda row: f\"{row['title']}. {row['text']}\"\n",
    "                if pd.notna(row.get('title')) and str(row.get('title', '')).strip()\n",
    "                else str(row.get('text', '')), axis=1)\n",
    "        df_fake['label'] = 0\n",
    "        dfs.append(df_fake[['text', 'label']])\n",
    "\n",
    "    if os.path.exists(true_path):\n",
    "        df_true = pd.read_csv(true_path)\n",
    "        print(f\"    外部数据集 True.csv: {len(df_true)} 条\")\n",
    "        if 'title' in df_true.columns:\n",
    "            df_true['text'] = df_true.apply(\n",
    "                lambda row: f\"{row['title']}. {row['text']}\"\n",
    "                if pd.notna(row.get('title')) and str(row.get('title', '')).strip()\n",
    "                else str(row.get('text', '')), axis=1)\n",
    "        df_true['label'] = 1\n",
    "        dfs.append(df_true[['text', 'label']])\n",
    "\n",
    "    if dfs:\n",
    "        df_extra = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"    外部数据集合计: {len(df_extra)} 条 \"\n",
    "              f\"(Fake: {(df_extra['label']==0).sum()}, Real: {(df_extra['label']==1).sum()})\")\n",
    "        return df_extra\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "\n",
    "# ========================= 多数据集合并 =========================\n",
    "def merge_datasets(df_list, dedup=True):\n",
    "    \"\"\"合并多个DataFrame, 可选去重\"\"\"\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    total_before = len(df)\n",
    "    if dedup:\n",
    "        df['_dedup_key'] = df['text'].astype(str).str[:100]\n",
    "        df = df.drop_duplicates(subset='_dedup_key', keep='first')\n",
    "        df = df.drop(columns='_dedup_key').reset_index(drop=True)\n",
    "    total_after = len(df)\n",
    "    print(f\"  合并数据集: {total_before} 条 → 去重后 {total_after} 条 (移除 {total_before - total_after} 条重复)\")\n",
    "    print(f\"  合并后标签分布: Fake={int((df['label']==0).sum())}, Real={int((df['label']==1).sum())}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "# ========================= 文本数据增强 (EDA) =========================\n",
    "def _random_deletion(words, p=0.1):\n",
    "    if len(words) <= 1:\n",
    "        return words\n",
    "    new_words = [w for w in words if random.random() > p]\n",
    "    return new_words if new_words else [random.choice(words)]\n",
    "\n",
    "def _random_swap(words, n=1):\n",
    "    if len(words) < 2:\n",
    "        return words\n",
    "    new_words = words.copy()\n",
    "    for _ in range(n):\n",
    "        idx1, idx2 = random.sample(range(len(new_words)), 2)\n",
    "        new_words[idx1], new_words[idx2] = new_words[idx2], new_words[idx1]\n",
    "    return new_words\n",
    "\n",
    "def augment_text(text, num_aug=1):\n",
    "    words = text.split()\n",
    "    if len(words) < 3:\n",
    "        return [text] * num_aug\n",
    "    augmented = []\n",
    "    for _ in range(num_aug):\n",
    "        method = random.choice(['delete', 'swap'])\n",
    "        if method == 'delete':\n",
    "            new_words = _random_deletion(words, p=0.1)\n",
    "        else:\n",
    "            n_swaps = max(1, len(words) // 20)\n",
    "            new_words = _random_swap(words, n=n_swaps)\n",
    "        augmented.append(' '.join(new_words))\n",
    "    return augmented\n",
    "\n",
    "def augment_dataset(X_train, y_train, num_aug=1, seed=42):\n",
    "    \"\"\"对训练集进行文本数据增强\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    X_list, y_list = list(X_train), list(y_train)\n",
    "    orig_size = len(X_list)\n",
    "    for i in range(orig_size):\n",
    "        aug_texts = augment_text(X_list[i], num_aug=num_aug)\n",
    "        for aug_text in aug_texts:\n",
    "            X_list.append(aug_text)\n",
    "            y_list.append(y_list[i])\n",
    "    combined = list(zip(X_list, y_list))\n",
    "    random.shuffle(combined)\n",
    "    X_aug, y_aug = zip(*combined)\n",
    "    print(f\"    数据增强: {orig_size} → {len(X_aug)} (+{len(X_aug) - orig_size} 条增强样本)\")\n",
    "    return np.array(X_aug), np.array(y_aug)\n",
    "\n",
    "print(\"✓ Data Augmentation module loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 5: Data Loading & Preprocessing Module (data_utils)\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "数据加载与预处理: 清洗 → 词汇表 → Dataset → GloVe\n",
    "\"\"\"\n",
    "import re\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ========================= 停用词列表 =========================\n",
    "STOP_WORDS = set([\n",
    "    'a', 'an', 'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',\n",
    "    'of', 'with', 'by', 'from', 'is', 'are', 'was', 'were', 'be', 'been',\n",
    "    'being', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would',\n",
    "    'could', 'should', 'may', 'might', 'shall', 'can', 'it', 'its',\n",
    "    'that', 'which', 'who', 'whom', 'this', 'these', 'those', 'am',\n",
    "    'i', 'me', 'my', 'we', 'our', 'you', 'your', 'he', 'him', 'his',\n",
    "    'she', 'her', 'they', 'them', 'their', 'as', 'if', 'when', 'than',\n",
    "    'so', 'no', 'not', 'up', 'out', 'about', 'into', 'over', 'after',\n",
    "    'before', 'between', 'under', 'again', 'then', 'once', 'here',\n",
    "    'there', 'where', 'how', 'all', 'both', 'each', 'more', 'other',\n",
    "    'some', 'such', 'own', 'same', 'just', 'now', 's', 't', 'd', 'm',\n",
    "])\n",
    "\n",
    "\n",
    "# ========================= 文本清洗 =========================\n",
    "def clean_text(text):\n",
    "    \"\"\"文本清洗: 小写 → 去HTML/URL → 只保留字母 → 去停用词\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', ' ', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    words = [w for w in text.split() if w not in STOP_WORDS and len(w) > 1]\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "# ========================= 词汇表 =========================\n",
    "class Vocabulary:\n",
    "    def __init__(self, max_vocab_size=20000):\n",
    "        self.word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<UNK>'}\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "\n",
    "    def build(self, texts):\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            counter.update(text.split())\n",
    "        for word, _ in counter.most_common(self.max_vocab_size - 2):\n",
    "            idx = len(self.word2idx)\n",
    "            self.word2idx[word] = idx\n",
    "            self.idx2word[idx] = word\n",
    "\n",
    "    def encode(self, text, max_length):\n",
    "        \"\"\"智能截断: 超长文本取前70%+后30%\"\"\"\n",
    "        tokens = text.split()\n",
    "        if len(tokens) > max_length:\n",
    "            head_len = int(max_length * 0.7)\n",
    "            tail_len = max_length - head_len\n",
    "            tokens = tokens[:head_len] + tokens[-tail_len:]\n",
    "        indices = [self.word2idx.get(w, 1) for w in tokens]\n",
    "        padding_len = max_length - len(indices)\n",
    "        if padding_len > 0:\n",
    "            indices = indices + [0] * padding_len\n",
    "        return indices\n",
    "\n",
    "    def decode(self, indices):\n",
    "        return ' '.join(self.idx2word.get(idx, '<UNK>') for idx in indices if idx != 0)\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "\n",
    "# ========================= PyTorch Dataset =========================\n",
    "class FakeNewsDataset(Dataset):\n",
    "    def __init__(self, texts, labels, vocab, max_length=500):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = vocab\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        indices = self.vocab.encode(self.texts[idx], self.max_length)\n",
    "        return (torch.tensor(indices, dtype=torch.long),\n",
    "                torch.tensor(self.labels[idx], dtype=torch.float))\n",
    "\n",
    "\n",
    "# ========================= 多数据集加载主函数 =========================\n",
    "def load_and_preprocess_multi_data(csv_path, extra_dataset_dir=None,\n",
    "                                   max_vocab_size=20000, max_length=500,\n",
    "                                   test_size=0.2, val_size=0.1,\n",
    "                                   random_state=42, augment=False, num_aug=1):\n",
    "    \"\"\"加载多个数据集 → 合并 → 清洗 → 划分 → (可选)增强 → 创建Dataset\"\"\"\n",
    "    print(f\"  加载主数据集: {csv_path}\")\n",
    "    df_main = pd.read_csv(csv_path)\n",
    "    print(f\"    主数据集: {len(df_main)} 条\")\n",
    "\n",
    "    df_list = [df_main]\n",
    "    if extra_dataset_dir is not None:\n",
    "        df_extra = load_news_dataset(extra_dataset_dir)\n",
    "        df_list.append(df_extra)\n",
    "\n",
    "    if len(df_list) > 1:\n",
    "        df = merge_datasets(df_list, dedup=True)\n",
    "    else:\n",
    "        df = df_main\n",
    "\n",
    "    df = df.dropna(subset=['text', 'label'])\n",
    "    print(f\"  开始文本清洗...\")\n",
    "    df['clean_text'] = df['text'].apply(clean_text)\n",
    "    df = df[df['clean_text'].str.len() > 0].reset_index(drop=True)\n",
    "    print(f\"  清洗后数据量: {len(df)}\")\n",
    "    print(f\"  标签分布: Fake={int((df['label']==0).sum())}, Real={int((df['label']==1).sum())}\")\n",
    "\n",
    "    texts = df['clean_text'].values\n",
    "    labels = df['label'].values.astype(int)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        texts, labels, test_size=test_size, random_state=random_state, stratify=labels)\n",
    "    val_ratio = val_size / (1 - test_size)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train, test_size=val_ratio, random_state=random_state, stratify=y_train)\n",
    "    print(f\"  训练集: {len(X_train)}, 验证集: {len(X_val)}, 测试集: {len(X_test)}\")\n",
    "\n",
    "    if augment and num_aug > 0:\n",
    "        print(f\"  ★ 对训练集进行数据增强 (num_aug={num_aug})...\")\n",
    "        X_train, y_train = augment_dataset(X_train, y_train, num_aug=num_aug, seed=random_state)\n",
    "        print(f\"  增强后训练集: {len(X_train)}\")\n",
    "\n",
    "    vocab = Vocabulary(max_vocab_size)\n",
    "    vocab.build(X_train)\n",
    "    print(f\"  词汇表大小: {vocab.vocab_size}\")\n",
    "\n",
    "    train_dataset = FakeNewsDataset(X_train, y_train, vocab, max_length)\n",
    "    val_dataset = FakeNewsDataset(X_val, y_val, vocab, max_length)\n",
    "    test_dataset = FakeNewsDataset(X_test, y_test, vocab, max_length)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, vocab\n",
    "\n",
    "\n",
    "# ========================= GloVe预训练词向量 =========================\n",
    "def load_glove_embeddings(glove_path, vocab, embed_dim=100):\n",
    "    \"\"\"加载GloVe预训练词向量, 构建embedding矩阵\"\"\"\n",
    "    print(f\"  加载GloVe词向量: {glove_path}\")\n",
    "    glove_dict = {}\n",
    "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            if word in vocab.word2idx:\n",
    "                vector = np.array(values[1:], dtype=np.float32)\n",
    "                if len(vector) == embed_dim:\n",
    "                    glove_dict[word] = vector\n",
    "\n",
    "    embedding_matrix = np.random.uniform(-0.25, 0.25, (vocab.vocab_size, embed_dim)).astype(np.float32)\n",
    "    embedding_matrix[0] = 0  # <PAD>\n",
    "\n",
    "    found = 0\n",
    "    for word, idx in vocab.word2idx.items():\n",
    "        if word in glove_dict:\n",
    "            embedding_matrix[idx] = glove_dict[word]\n",
    "            found += 1\n",
    "\n",
    "    coverage = found / vocab.vocab_size\n",
    "    print(f\"  GloVe覆盖率: {found}/{vocab.vocab_size} = {coverage:.2%}\")\n",
    "    return torch.tensor(embedding_matrix), coverage\n",
    "\n",
    "print(\"✓ Data Utils module loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 6: Model Definition (BiLSTM + Attention)\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "BiLSTM + Attention 分类器: 支持GloVe预训练词向量\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class BiLSTMAttentionClassifier(nn.Module):\n",
    "    \"\"\"双向LSTM + Attention 假新闻分类器\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim=100, hidden_dim=128,\n",
    "                 num_layers=2, dropout=0.5, pad_idx=0,\n",
    "                 pretrained_embeddings=None, freeze_embeddings=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "\n",
    "        # ★ 加载预训练词向量\n",
    "        if pretrained_embeddings is not None:\n",
    "            self.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "            if freeze_embeddings:\n",
    "                self.embedding.weight.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim, hidden_size=hidden_dim,\n",
    "            num_layers=num_layers, bidirectional=True,\n",
    "            dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
    "\n",
    "        lstm_out_dim = hidden_dim * 2  # 双向\n",
    "\n",
    "        # Attention (两层MLP)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(lstm_out_dim, hidden_dim), nn.Tanh(), nn.Linear(hidden_dim, 1))\n",
    "\n",
    "        # 分类头\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout), nn.Linear(lstm_out_dim, 64),\n",
    "            nn.ReLU(), nn.Dropout(0.3), nn.Linear(64, 1))\n",
    "\n",
    "        self.dropout_emb = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x, return_attention=False):\n",
    "        embedded = self.dropout_emb(self.embedding(x))\n",
    "        lstm_out, _ = self.lstm(embedded)\n",
    "\n",
    "        # Attention\n",
    "        attention_scores = self.attention(lstm_out).squeeze(-1)\n",
    "        mask = (x != 0).float()\n",
    "        attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
    "        attention_weights = torch.softmax(attention_scores, dim=1)\n",
    "        context = torch.bmm(attention_weights.unsqueeze(1), lstm_out).squeeze(1)\n",
    "\n",
    "        logits = self.fc(context).squeeze(-1)\n",
    "\n",
    "        if return_attention:\n",
    "            return logits, attention_weights\n",
    "        return logits\n",
    "\n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"✓ Model module loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 7: Trainer Module (Train & Evaluate)\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "训练与评估: FocalLoss, 学习率调度器, 早停, 梯度裁剪\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \"\"\"Focal Loss: 聚焦困难样本\"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce_loss = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        probs = torch.sigmoid(logits)\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "        focal_weight = (1 - p_t) ** self.gamma\n",
    "        return (self.alpha * focal_weight * bce_loss).mean()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(inputs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(labels.detach().cpu().numpy())\n",
    "    return total_loss / len(dataloader), accuracy_score(all_labels, all_preds)\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    metrics = {\n",
    "        'loss': avg_loss,\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds, zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_preds, zero_division=0),\n",
    "        'f1': f1_score(all_labels, all_preds, zero_division=0),\n",
    "        'confusion_matrix': confusion_matrix(all_labels, all_preds)\n",
    "    }\n",
    "    return metrics, all_preds, all_labels\n",
    "\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, test_dataset,\n",
    "                criterion, optimizer, device,\n",
    "                num_epochs=10, batch_size=32, use_scheduler=True):\n",
    "    \"\"\"完整训练流程: LR调度器 + 早停 + 保存最优模型\"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    scheduler = None\n",
    "    if use_scheduler:\n",
    "        scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, min_lr=1e-6)\n",
    "\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'test_acc': []}\n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    early_stop_patience = 5\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_metrics, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        test_metrics, _, _ = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_acc'].append(val_metrics['accuracy'])\n",
    "        history['test_acc'].append(test_metrics['accuracy'])\n",
    "\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"  Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Loss: {train_loss:.4f} | Train: {train_acc:.4f} | \"\n",
    "              f\"Val: {val_metrics['accuracy']:.4f}(F1:{val_metrics['f1']:.4f}) | \"\n",
    "              f\"Test: {test_metrics['accuracy']:.4f} | LR: {current_lr:.6f}\")\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_metrics['accuracy'])\n",
    "\n",
    "        if val_metrics['accuracy'] > best_val_acc:\n",
    "            best_val_acc = val_metrics['accuracy']\n",
    "            best_model_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"  Early stopping at epoch {epoch+1} (val acc not improving)\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return history, model\n",
    "\n",
    "print(\"✓ Trainer module loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 8: Visualization Module\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "可视化: 满足作业要求的全部图表 (Fig 1-8)\n",
    "\"\"\"\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix as cm_func\n",
    "\n",
    "matplotlib.rcParams['font.family'] = ['DejaVu Sans']\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='.*findfont.*')\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "def plot_training_curves(history, title=\"Training Curves\", save_path=None):\n",
    "    \"\"\"Fig 1/2: 训练损失 + 训练准确率 + 测试准确率\"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', color='tab:red', fontsize=12)\n",
    "    line1 = ax1.plot(epochs, history['train_loss'], 'r-o', label='Train Loss', markersize=4, linewidth=2)\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Accuracy', color='tab:blue', fontsize=12)\n",
    "    line2 = ax2.plot(epochs, history['train_acc'], 'b-s', label='Train Accuracy', markersize=4, linewidth=2)\n",
    "    line3 = ax2.plot(epochs, history['test_acc'], 'g-^', label='Test Accuracy', markersize=4, linewidth=2)\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    ax2.set_ylim([0, 1.05])\n",
    "    lines = line1 + line2 + line3\n",
    "    ax1.legend(lines, [l.get_label() for l in lines], loc='center right', fontsize=10)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"    Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_lr_comparison(histories_dict, title=\"LR Comparison\", save_path=None):\n",
    "    \"\"\"Fig 3/4: 不同学习率对比 (3个子图)\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(histories_dict)))\n",
    "    for idx, (lr, history) in enumerate(histories_dict.items()):\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "        c = colors[idx]\n",
    "        axes[0].plot(epochs, history['train_loss'], color=c, marker='o', markersize=3, linewidth=1.5, label=f'LR={lr}')\n",
    "        axes[1].plot(epochs, history['train_acc'], color=c, marker='s', markersize=3, linewidth=1.5, label=f'LR={lr}')\n",
    "        axes[2].plot(epochs, history['test_acc'], color=c, marker='^', markersize=3, linewidth=1.5, label=f'LR={lr}')\n",
    "    for ax, ylabel, subtitle in zip(axes, ['Loss', 'Accuracy', 'Accuracy'],\n",
    "                                     ['Training Loss', 'Training Accuracy', 'Test Accuracy']):\n",
    "        ax.set_xlabel('Epoch', fontsize=11); ax.set_ylabel(ylabel, fontsize=11)\n",
    "        ax.set_title(subtitle, fontsize=12); ax.legend(fontsize=9); ax.grid(True, alpha=0.3)\n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"    Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_batch_size_comparison(histories_dict, title=\"Batch Size Comparison\", save_path=None):\n",
    "    \"\"\"Fig 5/6: 不同batch size对比\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(histories_dict)))\n",
    "    for idx, (bs, history) in enumerate(histories_dict.items()):\n",
    "        epochs = range(1, len(history['train_loss']) + 1)\n",
    "        c = colors[idx]\n",
    "        axes[0].plot(epochs, history['train_loss'], color=c, marker='o', markersize=3, linewidth=1.5, label=f'BS={bs}')\n",
    "        axes[1].plot(epochs, history['train_acc'], color=c, marker='s', markersize=3, linewidth=1.5, label=f'BS={bs}')\n",
    "        axes[2].plot(epochs, history['test_acc'], color=c, marker='^', markersize=3, linewidth=1.5, label=f'BS={bs}')\n",
    "    for ax, ylabel, subtitle in zip(axes, ['Loss', 'Accuracy', 'Accuracy'],\n",
    "                                     ['Training Loss', 'Training Accuracy', 'Test Accuracy']):\n",
    "        ax.set_xlabel('Epoch', fontsize=11); ax.set_ylabel(ylabel, fontsize=11)\n",
    "        ax.set_title(subtitle, fontsize=12); ax.legend(fontsize=9); ax.grid(True, alpha=0.3)\n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"    Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_predictions_table(texts, true_labels, pred_labels, n=100, save_path=None):\n",
    "    \"\"\"Fig 7: 前n条测试集预测结果\"\"\"\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    n = min(n, len(texts))\n",
    "    label_map = {0: 'Fake', 1: 'Real'}\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8), gridspec_kw={'width_ratios': [1, 3]})\n",
    "    grid_size = int(np.ceil(np.sqrt(n)))\n",
    "    grid = np.zeros((grid_size, grid_size))\n",
    "    for i in range(n):\n",
    "        row, col = i // grid_size, i % grid_size\n",
    "        grid[row][col] = 1 if true_labels[i] == pred_labels[i] else -1\n",
    "    cmap = ListedColormap(['#ff6b6b', 'white', '#51cf66'])\n",
    "    axes[0].imshow(grid, cmap=cmap, vmin=-1, vmax=1, aspect='equal')\n",
    "    axes[0].set_title(f'Prediction Grid (First {n})\\nGreen=Correct, Red=Wrong', fontsize=11)\n",
    "    axes[1].axis('off')\n",
    "    show_n = min(20, n)\n",
    "    table_data, cell_colors = [], []\n",
    "    for i in range(show_n):\n",
    "        text_preview = texts[i][:60] + '...' if len(texts[i]) > 60 else texts[i]\n",
    "        true_l, pred_l = label_map[int(true_labels[i])], label_map[int(pred_labels[i])]\n",
    "        correct = 'Yes' if true_labels[i] == pred_labels[i] else 'No'\n",
    "        table_data.append([i+1, text_preview, true_l, pred_l, correct])\n",
    "        cell_colors.append(['#d4edda' if correct == 'Yes' else '#f8d7da'] * 5)\n",
    "    table = axes[1].table(cellText=table_data, colLabels=['#', 'Text', 'True', 'Pred', 'Correct'],\n",
    "                          cellColours=cell_colors, loc='upper center', cellLoc='left')\n",
    "    table.auto_set_font_size(False); table.set_fontsize(8); table.auto_set_column_width([0,1,2,3,4])\n",
    "    correct_count = sum(1 for i in range(n) if true_labels[i] == pred_labels[i])\n",
    "    fig.suptitle(f'Test Predictions: {correct_count}/{n} correct ({100*correct_count/n:.1f}%)',\n",
    "                 fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"    Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(true_labels, pred_labels, save_path=None):\n",
    "    \"\"\"Fig 8: 混淆矩阵\"\"\"\n",
    "    cm = cm_func(true_labels, pred_labels)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Fake (0)', 'Real (1)'], yticklabels=['Fake (0)', 'Real (1)'],\n",
    "                ax=ax, annot_kws={\"size\": 16})\n",
    "    ax.set_xlabel('Predicted Label', fontsize=12); ax.set_ylabel('True Label', fontsize=12)\n",
    "    ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"    Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"✓ Visualization module loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 9: Chain-of-Thought Reasoning Module\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "推理链 (CoT): 结合Attention权重 + 文本特征分析, 生成可解释的判断依据\n",
    "\"\"\"\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# ========================= 特征词典 =========================\n",
    "SENSATIONAL_WORDS = {\n",
    "    'shocking', 'unbelievable', 'breaking', 'exclusive', 'urgent',\n",
    "    'bombshell', 'horrifying', 'terrifying', 'scandal', 'outrage',\n",
    "    'devastating', 'explosive', 'stunning', 'alarming', 'incredible',\n",
    "    'exposed', 'secret', 'leaked', 'conspiracy', 'hoax', 'cover',\n",
    "    'destroyed', 'slammed', 'blasted', 'ripped', 'epic', 'insane'\n",
    "}\n",
    "CREDIBILITY_PHRASES = [\n",
    "    'according to', 'research shows', 'study finds', 'officials said',\n",
    "    'reuters', 'associated press', 'confirmed by', 'evidence suggests',\n",
    "    'data shows', 'report says', 'peer reviewed', 'investigation found',\n",
    "    'spokesperson said', 'published in', 'university of', 'department of',\n",
    "    'official statement', 'press release', 'government report'\n",
    "]\n",
    "EMOTIONAL_WORDS = {\n",
    "    'hate', 'love', 'angry', 'furious', 'amazing', 'terrible',\n",
    "    'disgusting', 'wonderful', 'horrible', 'fantastic', 'awful',\n",
    "    'outrageous', 'evil', 'hero', 'villain', 'miracle', 'disaster',\n",
    "    'tragic', 'brilliant', 'stupid', 'genius', 'idiot', 'corrupt',\n",
    "    'patriot', 'traitor', 'danger', 'threat', 'crisis', 'doom'\n",
    "}\n",
    "CLICKBAIT_PATTERNS = [\n",
    "    'you won', 'believe', 'click here', 'share this', 'going viral',\n",
    "    'mind blowing', 'what happened next', 'number', 'will shock',\n",
    "    'doctors hate', 'one weird trick', 'exposed', 'they don want'\n",
    "]\n",
    "\n",
    "\n",
    "class ChainOfThoughtAnalyzer:\n",
    "    \"\"\"推理链分析器: Attention + 文本特征 → 可解释推理过程\"\"\"\n",
    "\n",
    "    def __init__(self, model, vocab, device, max_length=500):\n",
    "        self.model = model\n",
    "        self.vocab = vocab\n",
    "        self.device = device\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def analyze(self, text, original_text=None):\n",
    "        display_text = original_text if original_text else text\n",
    "        self.model.eval()\n",
    "        indices = self.vocab.encode(text, self.max_length)\n",
    "        input_tensor = torch.tensor([indices], dtype=torch.long).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits, attention_weights = self.model(input_tensor, return_attention=True)\n",
    "        prob = torch.sigmoid(logits).item()\n",
    "        prediction = 'Real' if prob > 0.5 else 'Fake'\n",
    "        confidence = prob if prob > 0.5 else 1 - prob\n",
    "\n",
    "        attention = attention_weights.squeeze().cpu().numpy()\n",
    "        tokens = text.split()[:self.max_length]\n",
    "        word_attention = {}\n",
    "        for i, token in enumerate(tokens):\n",
    "            if i < len(attention):\n",
    "                word_attention[token] = max(word_attention.get(token, 0), attention[i])\n",
    "        top_words = sorted(word_attention.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        features = self._analyze_features(text)\n",
    "        reasoning = self._generate_reasoning(prediction, confidence, top_words, features)\n",
    "\n",
    "        return {\n",
    "            'text_preview': display_text[:300] + '...' if len(display_text) > 300 else display_text,\n",
    "            'prediction': prediction,\n",
    "            'confidence': f\"{confidence:.2%}\",\n",
    "            'top_attention_words': [(w, f\"{a:.4f}\") for w, a in top_words[:5]],\n",
    "            'text_features': features,\n",
    "            'reasoning_chain': reasoning\n",
    "        }\n",
    "\n",
    "    def _analyze_features(self, text):\n",
    "        words = text.split()\n",
    "        text_lower = text.lower()\n",
    "        sensational_found = [w for w in words if w in SENSATIONAL_WORDS]\n",
    "        sensational_ratio = len(sensational_found) / max(len(words), 1)\n",
    "        credibility_found = [p for p in CREDIBILITY_PHRASES if p in text_lower]\n",
    "        emotional_found = [w for w in words if w in EMOTIONAL_WORDS]\n",
    "        emotional_ratio = len(emotional_found) / max(len(words), 1)\n",
    "        clickbait_found = [p for p in CLICKBAIT_PATTERNS if p in text_lower]\n",
    "        avg_word_len = np.mean([len(w) for w in words]) if words else 0\n",
    "\n",
    "        def score(ratio, thresholds=(0.01, 0.005)):\n",
    "            if ratio > thresholds[0]: return 'HIGH'\n",
    "            elif ratio > thresholds[1]: return 'MEDIUM'\n",
    "            return 'LOW'\n",
    "\n",
    "        return {\n",
    "            'sensational_score': score(sensational_ratio),\n",
    "            'sensational_words': sensational_found[:5],\n",
    "            'credibility_score': 'HIGH' if len(credibility_found) >= 2 else 'MEDIUM' if len(credibility_found) == 1 else 'LOW',\n",
    "            'credibility_indicators': credibility_found[:3],\n",
    "            'emotional_score': score(emotional_ratio),\n",
    "            'emotional_words': emotional_found[:5],\n",
    "            'clickbait_score': 'HIGH' if len(clickbait_found) >= 2 else 'MEDIUM' if len(clickbait_found) == 1 else 'LOW',\n",
    "            'clickbait_patterns': clickbait_found[:3],\n",
    "            'text_length': len(words),\n",
    "            'avg_word_length': f\"{avg_word_len:.1f}\"\n",
    "        }\n",
    "\n",
    "    def _generate_reasoning(self, prediction, confidence, top_words, features):\n",
    "        lines = []\n",
    "        lines.append(\"=\" * 50)\n",
    "        lines.append(\"Step 1 - Text Feature Analysis:\")\n",
    "        lines.append(f\"  [Sensational Language] {features['sensational_score']}\")\n",
    "        if features['sensational_words']:\n",
    "            lines.append(f\"    Found: {', '.join(features['sensational_words'])}\")\n",
    "        lines.append(f\"  [Source Credibility]  {features['credibility_score']}\")\n",
    "        if features['credibility_indicators']:\n",
    "            lines.append(f\"    Found: {', '.join(features['credibility_indicators'])}\")\n",
    "        lines.append(f\"  [Emotional Tone]     {features['emotional_score']}\")\n",
    "        if features['emotional_words']:\n",
    "            lines.append(f\"    Found: {', '.join(features['emotional_words'])}\")\n",
    "        lines.append(f\"  [Clickbait Pattern]  {features['clickbait_score']}\")\n",
    "        if features['clickbait_patterns']:\n",
    "            lines.append(f\"    Found: {', '.join(features['clickbait_patterns'])}\")\n",
    "        lines.append(f\"  [Text Length]        {features['text_length']} words\")\n",
    "\n",
    "        lines.append(\"\\nStep 2 - Model Attention Key Words:\")\n",
    "        for word, weight in top_words[:5]:\n",
    "            bar = '█' * int(float(weight) * 500)\n",
    "            lines.append(f\"  '{word}' [{weight}] {bar}\")\n",
    "\n",
    "        lines.append(\"\\nStep 3 - Reasoning Chain:\")\n",
    "        reasons = []\n",
    "        if prediction == 'Fake':\n",
    "            if features['sensational_score'] in ('HIGH', 'MEDIUM'):\n",
    "                reasons.append(\"The text contains sensational/exaggerated language, common in fabricated news.\")\n",
    "            if features['credibility_score'] == 'LOW':\n",
    "                reasons.append(\"The text lacks references to credible sources, reducing its reliability.\")\n",
    "            if features['emotional_score'] in ('HIGH', 'MEDIUM'):\n",
    "                reasons.append(\"Highly emotional language is detected, often used to manipulate reader opinions.\")\n",
    "            if features['clickbait_score'] in ('HIGH', 'MEDIUM'):\n",
    "                reasons.append(\"Clickbait-style phrases are present, indicating engagement over accuracy.\")\n",
    "            if not reasons:\n",
    "                reasons.append(\"The model's learned patterns indicate this text matches characteristics of fake news.\")\n",
    "        else:\n",
    "            if features['credibility_score'] in ('HIGH', 'MEDIUM'):\n",
    "                reasons.append(\"The text references credible sources, consistent with legitimate news.\")\n",
    "            if features['sensational_score'] == 'LOW':\n",
    "                reasons.append(\"The text uses neutral, factual language consistent with professional journalism.\")\n",
    "            if features['emotional_score'] == 'LOW':\n",
    "                reasons.append(\"The text maintains an objective tone without excessive emotional manipulation.\")\n",
    "            if not reasons:\n",
    "                reasons.append(\"The model's learned patterns indicate this text matches characteristics of real news.\")\n",
    "        for i, reason in enumerate(reasons, 1):\n",
    "            lines.append(f\"  {i}. {reason}\")\n",
    "\n",
    "        lines.append(f\"\\n{'=' * 50}\")\n",
    "        lines.append(f\"Conclusion: This article is classified as [{prediction}] with {confidence:.2%} confidence.\")\n",
    "        lines.append(\"=\" * 50)\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "def batch_analyze(analyzer, texts, n=5):\n",
    "    \"\"\"对多条文本进行推理链分析\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"  Chain-of-Thought Reasoning Demo\")\n",
    "    print(\"=\" * 60)\n",
    "    for i in range(min(n, len(texts))):\n",
    "        print(f\"\\n{'─' * 60}\")\n",
    "        print(f\"  Sample {i+1}:\")\n",
    "        print(f\"{'─' * 60}\")\n",
    "        result = analyzer.analyze(texts[i])\n",
    "        print(f\"\\nText Preview: {result['text_preview']}\")\n",
    "        print(f\"\\nPrediction: {result['prediction']} (Confidence: {result['confidence']})\")\n",
    "        print(f\"\\n{result['reasoning_chain']}\")\n",
    "    print(f\"\\n{'═' * 60}\")\n",
    "\n",
    "print(\"✓ Chain-of-Thought module loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Configuration & Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 10: Configuration & Helper Functions\n",
    "# ============================================================\n",
    "\n",
    "# 默认超参数 (GloVe优化版)\n",
    "CONFIG = {\n",
    "    'embed_dim': 100,           # GloVe维度\n",
    "    'hidden_dim': 128,          # LSTM隐藏层维度\n",
    "    'num_layers': 2,            # LSTM层数\n",
    "    'dropout': 0.5,             # Dropout比率\n",
    "    'max_vocab_size': 20000,    # 词汇表大小\n",
    "    'max_length': 500,          # 文本最大长度 (token数)\n",
    "    'num_epochs': 20,           # 最大训练轮数\n",
    "    'batch_size': 32,           # 批大小\n",
    "    'learning_rate': 0.001,     # 学习率\n",
    "    'weight_decay': 1e-4,       # 权重衰减 (L2正则化)\n",
    "    'freeze_embeddings': True,  # 冻结GloVe嵌入层\n",
    "}\n",
    "\n",
    "# 全局变量\n",
    "_glove_matrix = None\n",
    "_vocab = None\n",
    "\n",
    "\n",
    "def create_model(vocab_size, glove_matrix=None):\n",
    "    \"\"\"创建新的模型实例 (带GloVe)\"\"\"\n",
    "    model = BiLSTMAttentionClassifier(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=CONFIG['embed_dim'],\n",
    "        hidden_dim=CONFIG['hidden_dim'],\n",
    "        num_layers=CONFIG['num_layers'],\n",
    "        dropout=CONFIG['dropout'],\n",
    "        pretrained_embeddings=glove_matrix,\n",
    "        freeze_embeddings=CONFIG['freeze_embeddings']\n",
    "    )\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "\n",
    "def run_experiment(train_ds, val_ds, test_ds, vocab_size,\n",
    "                   loss_fn='bce', learning_rate=None, batch_size=None,\n",
    "                   num_epochs=None, use_scheduler=True, glove_matrix=None):\n",
    "    \"\"\"运行单次训练实验\"\"\"\n",
    "    lr = learning_rate or CONFIG['learning_rate']\n",
    "    bs = batch_size or CONFIG['batch_size']\n",
    "    epochs = num_epochs or CONFIG['num_epochs']\n",
    "\n",
    "    model = create_model(vocab_size, glove_matrix)\n",
    "\n",
    "    if loss_fn == 'bce':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif loss_fn == 'focal':\n",
    "        criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss: {loss_fn}\")\n",
    "    criterion = criterion.to(DEVICE)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=lr, weight_decay=CONFIG['weight_decay'])\n",
    "\n",
    "    history, best_model = train_model(\n",
    "        model, train_ds, val_ds, test_ds,\n",
    "        criterion, optimizer, DEVICE,\n",
    "        num_epochs=epochs, batch_size=bs, use_scheduler=use_scheduler)\n",
    "\n",
    "    return history, best_model\n",
    "\n",
    "print(\"✓ Configuration loaded\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Config: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Data + GloVe Embeddings\n",
    "\n",
    "This will:\n",
    "1. Load the main dataset (`fakenews 2.csv`)\n",
    "2. Load the extra dataset (`News_dataset/`)\n",
    "3. Merge & deduplicate → ~50K samples\n",
    "4. Apply text data augmentation → ~60K training samples\n",
    "5. Load GloVe pre-trained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 11: Load Data + GloVe\n",
    "# ============================================================\n",
    "import time\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"  Fake News Detection with Deep Learning\")\n",
    "print(\"  BiLSTM + Attention + GloVe + Chain-of-Thought\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n[Step 1/7] Loading data and GloVe embeddings...\")\n",
    "t0 = time.time()\n",
    "\n",
    "train_ds, val_ds, test_ds, vocab = load_and_preprocess_multi_data(\n",
    "    DATA_PATH,\n",
    "    extra_dataset_dir=EXTRA_DATA_DIR,\n",
    "    max_vocab_size=CONFIG['max_vocab_size'],\n",
    "    max_length=CONFIG['max_length'],\n",
    "    augment=True,     # ★ 开启文本数据增强\n",
    "    num_aug=1,        # 每条训练样本生成1条增强样本\n",
    ")\n",
    "_vocab = vocab\n",
    "vocab_size = vocab.vocab_size\n",
    "\n",
    "# ★ 加载GloVe预训练词向量\n",
    "glove_matrix = None\n",
    "if os.path.exists(GLOVE_PATH):\n",
    "    glove_matrix, coverage = load_glove_embeddings(GLOVE_PATH, vocab, embed_dim=CONFIG['embed_dim'])\n",
    "    _glove_matrix = glove_matrix\n",
    "    print(f\"  ★ GloVe loaded! Coverage: {coverage:.2%}\")\n",
    "else:\n",
    "    print(f\"  ⚠ GloVe not found: {GLOVE_PATH}\")\n",
    "\n",
    "print(f\"\\n  ✓ Data loaded in {time.time()-t0:.1f}s\")\n",
    "print(f\"  Train: {len(train_ds)}, Val: {len(val_ds)}, Test: {len(test_ds)}\")\n",
    "print(f\"  Vocabulary: {vocab_size} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Experiments (Fig 1-6)\n",
    "\n",
    "Each experiment trains a fresh model with different hyperparameters.\n",
    "With GPU, each experiment takes ~2-5 minutes (vs 30+ min on CPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 12: Experiment 1 - BCE Loss (Default Config) → Fig 1\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"[Step 2/7] Experiment 1: BCE Loss (default config)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "t0 = time.time()\n",
    "history_bce, model_bce = run_experiment(\n",
    "    train_ds, val_ds, test_ds, vocab_size,\n",
    "    loss_fn='bce', glove_matrix=glove_matrix\n",
    ")\n",
    "print(f\"  Done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "plot_training_curves(\n",
    "    history_bce,\n",
    "    title=\"Fig 1: BCE Loss - Default Config (GloVe)\",\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig1_bce_default.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 13: Experiment 2 - Focal Loss (Default Config) → Fig 2\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"[Step 3/7] Experiment 2: Focal Loss (default config)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "t0 = time.time()\n",
    "history_focal, model_focal = run_experiment(\n",
    "    train_ds, val_ds, test_ds, vocab_size,\n",
    "    loss_fn='focal', glove_matrix=glove_matrix\n",
    ")\n",
    "print(f\"  Done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "plot_training_curves(\n",
    "    history_focal,\n",
    "    title=\"Fig 2: Focal Loss - Default Config (GloVe)\",\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig2_focal_default.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 14: Experiment 3 - Learning Rate Comparison → Fig 3 & 4\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"[Step 4/7] Experiment 3: Learning Rate Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "# --- Fig 3: BCE + 不同LR ---\n",
    "print(\"\\n  --- BCE Loss ---\")\n",
    "lr_histories_bce = {}\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n  LR = {lr}:\")\n",
    "    h, _ = run_experiment(train_ds, val_ds, test_ds, vocab_size,\n",
    "                          loss_fn='bce', learning_rate=lr, glove_matrix=glove_matrix)\n",
    "    lr_histories_bce[lr] = h\n",
    "\n",
    "plot_lr_comparison(\n",
    "    lr_histories_bce,\n",
    "    title=\"Fig 3: BCE Loss - Learning Rate Comparison\",\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig3_lr_bce.png\")\n",
    ")\n",
    "\n",
    "# --- Fig 4: Focal + 不同LR ---\n",
    "print(\"\\n  --- Focal Loss ---\")\n",
    "lr_histories_focal = {}\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n  LR = {lr}:\")\n",
    "    h, _ = run_experiment(train_ds, val_ds, test_ds, vocab_size,\n",
    "                          loss_fn='focal', learning_rate=lr, glove_matrix=glove_matrix)\n",
    "    lr_histories_focal[lr] = h\n",
    "\n",
    "plot_lr_comparison(\n",
    "    lr_histories_focal,\n",
    "    title=\"Fig 4: Focal Loss - Learning Rate Comparison\",\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig4_lr_focal.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 15: Experiment 4 - Batch Size Comparison → Fig 5 & 6\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"[Step 5/7] Experiment 4: Batch Size Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "\n",
    "# --- Fig 5: BCE + 不同BS ---\n",
    "print(\"\\n  --- BCE Loss ---\")\n",
    "bs_histories_bce = {}\n",
    "for bs in batch_sizes:\n",
    "    print(f\"\\n  Batch Size = {bs}:\")\n",
    "    h, _ = run_experiment(train_ds, val_ds, test_ds, vocab_size,\n",
    "                          loss_fn='bce', batch_size=bs, glove_matrix=glove_matrix)\n",
    "    bs_histories_bce[bs] = h\n",
    "\n",
    "plot_batch_size_comparison(\n",
    "    bs_histories_bce,\n",
    "    title=\"Fig 5: BCE Loss - Batch Size Comparison\",\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig5_bs_bce.png\")\n",
    ")\n",
    "\n",
    "# --- Fig 6: Focal + 不同BS ---\n",
    "print(\"\\n  --- Focal Loss ---\")\n",
    "bs_histories_focal = {}\n",
    "for bs in batch_sizes:\n",
    "    print(f\"\\n  Batch Size = {bs}:\")\n",
    "    h, _ = run_experiment(train_ds, val_ds, test_ds, vocab_size,\n",
    "                          loss_fn='focal', batch_size=bs, glove_matrix=glove_matrix)\n",
    "    bs_histories_focal[bs] = h\n",
    "\n",
    "plot_batch_size_comparison(\n",
    "    bs_histories_focal,\n",
    "    title=\"Fig 6: Focal Loss - Batch Size Comparison\",\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig6_bs_focal.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Final Evaluation (Fig 7 & 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 16: Final Evaluation → Fig 7 (Predictions) & Fig 8 (Confusion Matrix)\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"[Step 6/7] Generating prediction visualizations...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 使用BCE模型的最优结果进行预测\n",
    "criterion_eval = nn.BCEWithLogitsLoss()\n",
    "test_loader = DataLoader(test_ds, batch_size=64, shuffle=False)\n",
    "test_metrics, all_preds, all_labels = evaluate(model_bce, test_loader, criterion_eval, DEVICE)\n",
    "\n",
    "print(f\"\\n  === Final Test Results (Best BCE + GloVe Model) ===\")\n",
    "print(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "print(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
    "print(f\"  F1 Score:  {test_metrics['f1']:.4f}\")\n",
    "print(f\"  Confusion Matrix:\\n{test_metrics['confusion_matrix']}\")\n",
    "\n",
    "# Fig 7: 前100条预测结果\n",
    "plot_predictions_table(\n",
    "    list(test_ds.texts[:100]),\n",
    "    all_labels[:100], all_preds[:100], n=100,\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig7_predictions.png\")\n",
    ")\n",
    "\n",
    "# Fig 8: 混淆矩阵\n",
    "plot_confusion_matrix(\n",
    "    all_labels, all_preds,\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig8_confusion_matrix.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Chain-of-Thought Reasoning Demo\n",
    "\n",
    "The CoT module analyzes:\n",
    "1. **Text features**: sensational language, source credibility, emotional tone, clickbait patterns\n",
    "2. **Model attention weights**: which words the model focused on most\n",
    "3. **Reasoning chain**: step-by-step logical explanation for the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 17: Chain-of-Thought Reasoning Demo\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"[Step 7/7] Chain-of-Thought Reasoning Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cot_analyzer = ChainOfThoughtAnalyzer(model_bce, vocab, DEVICE, CONFIG['max_length'])\n",
    "\n",
    "# 对测试集前5条进行推理链分析\n",
    "batch_analyze(cot_analyzer, list(test_ds.texts), n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Outputs to Google Drive (Optional)\n",
    "\n",
    "Copy all output figures to your Google Drive for safekeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 18: Save outputs to Google Drive\n",
    "# ============================================================\n",
    "import shutil\n",
    "\n",
    "drive_output = os.path.join(DATA_ROOT, 'outputs')\n",
    "os.makedirs(drive_output, exist_ok=True)\n",
    "\n",
    "# 复制所有输出图片到 Google Drive\n",
    "for fname in os.listdir(OUTPUT_DIR):\n",
    "    if fname.endswith('.png'):\n",
    "        src = os.path.join(OUTPUT_DIR, fname)\n",
    "        dst = os.path.join(drive_output, fname)\n",
    "        shutil.copy2(src, dst)\n",
    "        print(f\"  ✓ Saved: {dst}\")\n",
    "\n",
    "print(f\"\\n  All {len(os.listdir(drive_output))} figures saved to Google Drive!\")\n",
    "print(f\"  Location: {drive_output}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  ✅ All experiments completed!\")\n",
    "print(\"  Generated figures: fig1 ~ fig8\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
