{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî¨ Fake News Detection with DistilBERT\n",
    "## CDS525 Group Project ‚Äî Transformer-based Approach\n",
    "\n",
    "**Model**: DistilBERT (Distilled BERT) ‚Äî 40% smaller, 60% faster, retains 97% of BERT's performance\n",
    "\n",
    "**Key Features**:\n",
    "- ‚òÖ **Pre-trained Transformer**: DistilBERT-base-uncased (66M params) with fine-tuning\n",
    "- ‚òÖ **Merged Dataset**: Original (5K) + News_dataset (45K) ‚âà 50K samples\n",
    "- ‚òÖ **Text Data Augmentation** (EDA: Random Deletion / Swap)\n",
    "- ‚òÖ **Attention-based Explainability**: Extract DistilBERT attention weights for CoT reasoning\n",
    "- ‚òÖ **Training Strategy**: AdamW + Linear Warmup + Weight Decay + Early Stopping\n",
    "- ‚òÖ **Comprehensive Evaluation**: Accuracy, F1, ROC-AUC, Confusion Matrix\n",
    "- ‚òÖ **8 Required Figures** + Chain-of-Thought Reasoning Demo\n",
    "\n",
    "**Reference**: Training strategy inspired by `with-new-data.ipynb` (BERT/DistilBERT approach)\n",
    "\n",
    "**‚ö† REQUIRED**: GPU Runtime ‚Üí Runtime ‚Üí Change runtime type ‚Üí **T4 GPU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 1: Environment Setup & Install Dependencies\n",
    "# ============================================================\n",
    "%pip install -q transformers accelerate\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö† No GPU! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nUsing: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 2: Mount Google Drive & Set Data Paths\n",
    "# ============================================================\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "\n",
    "# ‚òÖ ‰øÆÊîπËøôÈáåÊåáÂêë‰Ω† Google Drive ‰∏≠ÁöÑÊï∞ÊçÆÁõÆÂΩï\n",
    "DATA_ROOT = '/content/drive/MyDrive/fakenews'\n",
    "\n",
    "DATA_PATH = os.path.join(DATA_ROOT, 'fakenews 2.csv')\n",
    "EXTRA_DATA_DIR = os.path.join(DATA_ROOT, 'News _dataset')\n",
    "OUTPUT_DIR = '/content/outputs_bert'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# È™åËØÅÊñá‰ª∂\n",
    "for p in [DATA_PATH,\n",
    "          os.path.join(EXTRA_DATA_DIR, 'Fake.csv'),\n",
    "          os.path.join(EXTRA_DATA_DIR, 'True.csv')]:\n",
    "    if os.path.exists(p):\n",
    "        print(f\"  ‚úì {os.path.basename(p)} ({os.path.getsize(p)/1e6:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"  ‚úó NOT FOUND: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading, Merging & Preprocessing\n",
    "\n",
    "**BERT preprocessing is minimal** (compared to LSTM):\n",
    "- Preserve punctuation, stopwords ‚Üí BERT's self-attention needs them for context\n",
    "- Only remove URLs, HTML tags\n",
    "- Remove \"Reuters\" data leakage (as found in reference notebook)\n",
    "- BERT tokenizer handles subword tokenization automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 3: Data Loading & Merging & Minimal BERT Cleaning\n",
    "# ============================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ========================= BERT-specific Text Cleaning =========================\n",
    "def clean_for_bert(text):\n",
    "    \"\"\"\n",
    "    Minimal cleaning for BERT:\n",
    "    - BERT's self-attention NEEDS punctuation, stopwords, and sentence structure\n",
    "    - Only remove noise: URLs, HTML tags, Reuters leakage\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # remove URLs\n",
    "    text = re.sub(r'<.*?>', '', text)                    # remove HTML\n",
    "    text = text.replace('reuters', '')                   # remove Reuters leakage\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()             # normalize spaces\n",
    "    return text\n",
    "\n",
    "\n",
    "# ========================= Load Extra Dataset =========================\n",
    "def load_news_dataset(dataset_dir):\n",
    "    \"\"\"Load News_dataset (Fake.csv + True.csv)\"\"\"\n",
    "    dfs = []\n",
    "    for fname, label in [('Fake.csv', 0), ('True.csv', 1)]:\n",
    "        fpath = os.path.join(dataset_dir, fname)\n",
    "        if os.path.exists(fpath):\n",
    "            df_part = pd.read_csv(fpath)\n",
    "            print(f\"    {fname}: {len(df_part)} rows\")\n",
    "            if 'title' in df_part.columns:\n",
    "                df_part['text'] = df_part.apply(\n",
    "                    lambda r: f\"{r['title']}. {r['text']}\"\n",
    "                    if pd.notna(r.get('title')) and str(r.get('title', '')).strip()\n",
    "                    else str(r.get('text', '')), axis=1)\n",
    "            df_part['label'] = label\n",
    "            dfs.append(df_part[['text', 'label']])\n",
    "    if dfs:\n",
    "        df = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"    Total: {len(df)} (Fake: {(df['label']==0).sum()}, Real: {(df['label']==1).sum()})\")\n",
    "        return df\n",
    "    return pd.DataFrame(columns=['text', 'label'])\n",
    "\n",
    "\n",
    "# ========================= EDA Data Augmentation =========================\n",
    "def _random_deletion(words, p=0.1):\n",
    "    if len(words) <= 1: return words\n",
    "    new = [w for w in words if random.random() > p]\n",
    "    return new if new else [random.choice(words)]\n",
    "\n",
    "def _random_swap(words, n=1):\n",
    "    if len(words) < 2: return words\n",
    "    w = words.copy()\n",
    "    for _ in range(n):\n",
    "        i, j = random.sample(range(len(w)), 2)\n",
    "        w[i], w[j] = w[j], w[i]\n",
    "    return w\n",
    "\n",
    "def augment_dataset(texts, labels, num_aug=1, seed=42):\n",
    "    \"\"\"EDA augmentation on training set\"\"\"\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    X, y = list(texts), list(labels)\n",
    "    orig = len(X)\n",
    "    for i in range(orig):\n",
    "        words = X[i].split()\n",
    "        if len(words) < 3: continue\n",
    "        for _ in range(num_aug):\n",
    "            method = random.choice(['delete', 'swap'])\n",
    "            if method == 'delete':\n",
    "                new_words = _random_deletion(words, p=0.1)\n",
    "            else:\n",
    "                new_words = _random_swap(words, n=max(1, len(words)//20))\n",
    "            X.append(' '.join(new_words))\n",
    "            y.append(y[i])\n",
    "    combined = list(zip(X, y)); random.shuffle(combined)\n",
    "    X_aug, y_aug = zip(*combined)\n",
    "    print(f\"    Augmentation: {orig} ‚Üí {len(X_aug)} (+{len(X_aug)-orig})\")\n",
    "    return np.array(X_aug), np.array(y_aug)\n",
    "\n",
    "\n",
    "# ========================= Main Data Pipeline =========================\n",
    "print(\"=\" * 60)\n",
    "print(\"  Loading & Preparing Data for DistilBERT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Load main dataset\n",
    "print(f\"\\n  Main dataset: {DATA_PATH}\")\n",
    "df_main = pd.read_csv(DATA_PATH)\n",
    "print(f\"    Rows: {len(df_main)}\")\n",
    "\n",
    "# 2. Load extra dataset\n",
    "df_list = [df_main]\n",
    "if os.path.exists(EXTRA_DATA_DIR):\n",
    "    print(f\"\\n  Extra dataset: {EXTRA_DATA_DIR}\")\n",
    "    df_extra = load_news_dataset(EXTRA_DATA_DIR)\n",
    "    df_list.append(df_extra)\n",
    "\n",
    "# 3. Merge & dedup\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "before = len(df)\n",
    "df['_key'] = df['text'].astype(str).str[:100]\n",
    "df = df.drop_duplicates(subset='_key', keep='first').drop(columns='_key').reset_index(drop=True)\n",
    "print(f\"\\n  Merged: {before} ‚Üí {len(df)} (removed {before-len(df)} duplicates)\")\n",
    "print(f\"  Labels: Fake={int((df['label']==0).sum())}, Real={int((df['label']==1).sum())}\")\n",
    "\n",
    "# 4. Clean for BERT (minimal)\n",
    "df = df.dropna(subset=['text', 'label'])\n",
    "print(f\"\\n  Applying BERT-minimal cleaning...\")\n",
    "df['clean_text'] = df['text'].apply(clean_for_bert)\n",
    "df = df[df['clean_text'].str.strip().str.len() > 0].reset_index(drop=True)\n",
    "print(f\"  After cleaning: {len(df)} rows\")\n",
    "\n",
    "# 5. Train/Val/Test split (80/10/10)\n",
    "texts = df['clean_text'].values\n",
    "labels = df['label'].values.astype(int)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=42, stratify=labels)\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"\\n  Split: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "\n",
    "# 6. Augment training set\n",
    "print(f\"\\n  Data augmentation...\")\n",
    "X_train, y_train = augment_dataset(X_train, y_train, num_aug=1, seed=42)\n",
    "\n",
    "print(f\"\\n  ‚úì Data ready!\")\n",
    "print(f\"  Final: Train={len(X_train)}, Val={len(X_val)}, Test={len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: BERT Tokenization and PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 4: Tokenization & Dataset Creation\n",
    "# ============================================================\n",
    "from transformers import DistilBertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import time\n",
    "\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "MAX_LEN = 256  # covers ~95% of articles; DistilBERT max is 512\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer loaded: {MODEL_NAME} (vocab: {tokenizer.vocab_size})\")\n",
    "\n",
    "\n",
    "class FakeNewsDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for BERT-style models\"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(\n",
    "            str(self.texts[idx]),\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        }\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating PyTorch datasets...\")\n",
    "t0 = time.time()\n",
    "\n",
    "train_dataset = FakeNewsDataset(X_train, y_train, tokenizer, MAX_LEN)\n",
    "val_dataset   = FakeNewsDataset(X_val, y_val, tokenizer, MAX_LEN)\n",
    "test_dataset  = FakeNewsDataset(X_test, y_test, tokenizer, MAX_LEN)\n",
    "\n",
    "print(f\"  Train: {len(train_dataset)}, Val: {len(val_dataset)}, Test: {len(test_dataset)}\")\n",
    "print(f\"  Max token length: {MAX_LEN}\")\n",
    "print(f\"  Done in {time.time()-t0:.1f}s\")\n",
    "\n",
    "# Quick check\n",
    "sample = train_dataset[0]\n",
    "print(f\"\\n  Sample input_ids shape: {sample['input_ids'].shape}\")\n",
    "print(f\"  Sample tokens: {tokenizer.decode(sample['input_ids'][:30])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Model, Trainer, Visualization & CoT Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 5: DistilBERT Classifier Model\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "DistilBERT + Classification Head\n",
    "- Load pre-trained DistilBERT (66M params)\n",
    "- Extract [CLS] token representation\n",
    "- Add Dropout + Dense layer for binary classification\n",
    "- output_attentions=True for Chain-of-Thought explainability\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "\n",
    "class DistilBertClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    DistilBERT for Fake News Detection\n",
    "    Architecture: DistilBERT ‚Üí [CLS] ‚Üí Dropout ‚Üí Dense ‚Üí Sigmoid\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name='distilbert-base-uncased', dropout=0.2, freeze_bert=False):\n",
    "        super().__init__()\n",
    "        self.distilbert = DistilBertModel.from_pretrained(model_name)\n",
    "\n",
    "        if freeze_bert:\n",
    "            for param in self.distilbert.parameters():\n",
    "                param.requires_grad = False\n",
    "            print(\"  ‚òÖ DistilBERT layers FROZEN (only training classifier head)\")\n",
    "\n",
    "        hidden_size = self.distilbert.config.hidden_size  # 768\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, return_attention=False):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        Returns logits (and optionally attention weights for CoT).\n",
    "        \"\"\"\n",
    "        outputs = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=return_attention\n",
    "        )\n",
    "        # [CLS] token is the first token\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # (batch, 768)\n",
    "        logits = self.classifier(cls_output).squeeze(-1)  # (batch,)\n",
    "\n",
    "        if return_attention:\n",
    "            # attentions: tuple of (batch, num_heads, seq_len, seq_len) per layer\n",
    "            return logits, outputs.attentions\n",
    "        return logits\n",
    "\n",
    "    def count_parameters(self):\n",
    "        total = sum(p.numel() for p in self.parameters())\n",
    "        trainable = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        return total, trainable\n",
    "\n",
    "\n",
    "print(\"‚úì DistilBertClassifier defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 6: Trainer (Train / Evaluate / Focal Loss)\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "Training loop for DistilBERT:\n",
    "- AdamW with linear warmup (standard for Transformers)\n",
    "- Gradient clipping\n",
    "- Early stopping on validation accuracy\n",
    "- Focal Loss option\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix as cm_func, roc_curve, auc\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce = nn.functional.binary_cross_entropy_with_logits(logits, targets, reduction='none')\n",
    "        probs = torch.sigmoid(logits)\n",
    "        p_t = probs * targets + (1 - probs) * (1 - targets)\n",
    "        return (self.alpha * (1 - p_t) ** self.gamma * bce).mean()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, dataloader, optimizer, scheduler, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        ids = batch['input_ids'].to(device)\n",
    "        mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(ids, mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        preds = (torch.sigmoid(logits) > 0.5).float()\n",
    "        all_preds.extend(preds.detach().cpu().numpy())\n",
    "        all_labels.extend(labels.detach().cpu().numpy())\n",
    "\n",
    "    return total_loss / len(dataloader), accuracy_score(all_labels, all_preds)\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            ids = batch['input_ids'].to(device)\n",
    "            mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            logits = model(ids, mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "            preds = (probs > 0.5).float()\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "    metrics = {\n",
    "        'loss': total_loss / len(dataloader),\n",
    "        'accuracy': accuracy_score(all_labels, all_preds),\n",
    "        'precision': precision_score(all_labels, all_preds, zero_division=0),\n",
    "        'recall': recall_score(all_labels, all_preds, zero_division=0),\n",
    "        'f1': f1_score(all_labels, all_preds, zero_division=0),\n",
    "        'confusion_matrix': cm_func(all_labels, all_preds)\n",
    "    }\n",
    "    return metrics, all_preds, all_labels, all_probs\n",
    "\n",
    "\n",
    "def train_bert_model(model, train_ds, val_ds, test_ds,\n",
    "                     criterion, device,\n",
    "                     num_epochs=3, batch_size=32, learning_rate=2e-5,\n",
    "                     weight_decay=0.01, warmup_ratio=0.1):\n",
    "    \"\"\"Full training loop with warmup scheduler and early stopping\"\"\"\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "    # AdamW (standard for Transformers)\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Linear warmup scheduler\n",
    "    total_steps = len(train_loader) * num_epochs\n",
    "    warmup_steps = int(total_steps * warmup_ratio)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n",
    "\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': [], 'test_acc': []}\n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "    patience_counter = 0\n",
    "    early_stop_patience = 3\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        t0 = time.time()\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, scheduler, criterion, device)\n",
    "        val_metrics, _, _, _ = evaluate_model(model, val_loader, criterion, device)\n",
    "        test_metrics, _, _, _ = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_acc'].append(val_metrics['accuracy'])\n",
    "        history['test_acc'].append(test_metrics['accuracy'])\n",
    "\n",
    "        lr_now = optimizer.param_groups[0]['lr']\n",
    "        elapsed = time.time() - t0\n",
    "\n",
    "        print(f\"  Epoch [{epoch+1}/{num_epochs}] ({elapsed:.0f}s) \"\n",
    "              f\"Loss: {train_loss:.4f} | Train: {train_acc:.4f} | \"\n",
    "              f\"Val: {val_metrics['accuracy']:.4f}(F1:{val_metrics['f1']:.4f}) | \"\n",
    "              f\"Test: {test_metrics['accuracy']:.4f} | LR: {lr_now:.2e}\")\n",
    "\n",
    "        if val_metrics['accuracy'] > best_val_acc:\n",
    "            best_val_acc = val_metrics['accuracy']\n",
    "            best_model_state = {k: v.clone() for k, v in model.state_dict().items()}\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"  Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return history, model\n",
    "\n",
    "\n",
    "print(\"‚úì Trainer module defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 7: Visualization Module (All 8 Figures + ROC-AUC)\n",
    "# ============================================================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix as cm_func, roc_curve, auc\n",
    "\n",
    "matplotlib.rcParams['font.family'] = ['DejaVu Sans']\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='.*findfont.*')\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\n",
    "def plot_training_curves(history, title=\"Training Curves\", save_path=None):\n",
    "    \"\"\"Fig 1/2: Loss + Train Acc + Test Acc\"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax1.set_xlabel('Epoch', fontsize=12)\n",
    "    ax1.set_ylabel('Loss', color='tab:red', fontsize=12)\n",
    "    l1 = ax1.plot(epochs, history['train_loss'], 'r-o', label='Train Loss', markersize=5, linewidth=2)\n",
    "    if 'val_loss' in history:\n",
    "        l1b = ax1.plot(epochs, history['val_loss'], 'r--s', label='Val Loss', markersize=4, linewidth=1.5, alpha=0.7)\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Accuracy', fontsize=12)\n",
    "    l2 = ax2.plot(epochs, history['train_acc'], 'b-s', label='Train Acc', markersize=5, linewidth=2)\n",
    "    l3 = ax2.plot(epochs, history['val_acc'], 'c-D', label='Val Acc', markersize=4, linewidth=1.5)\n",
    "    l4 = ax2.plot(epochs, history['test_acc'], 'g-^', label='Test Acc', markersize=5, linewidth=2)\n",
    "    ax2.set_ylim([0.4, 1.05])\n",
    "    lines = l1 + l2 + l3 + l4\n",
    "    ax1.legend(lines, [l.get_label() for l in lines], loc='center right', fontsize=9)\n",
    "    plt.title(title, fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight'); print(f\"    Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_lr_comparison(histories_dict, title=\"LR Comparison\", save_path=None):\n",
    "    \"\"\"Fig 3/4: Different LR comparison\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(histories_dict)))\n",
    "    for idx, (lr, h) in enumerate(histories_dict.items()):\n",
    "        ep = range(1, len(h['train_loss']) + 1)\n",
    "        c = colors[idx]\n",
    "        axes[0].plot(ep, h['train_loss'], color=c, marker='o', ms=3, lw=1.5, label=f'LR={lr}')\n",
    "        axes[1].plot(ep, h['train_acc'], color=c, marker='s', ms=3, lw=1.5, label=f'LR={lr}')\n",
    "        axes[2].plot(ep, h['test_acc'], color=c, marker='^', ms=3, lw=1.5, label=f'LR={lr}')\n",
    "    for ax, yl, t in zip(axes, ['Loss', 'Accuracy', 'Accuracy'],\n",
    "                          ['Training Loss', 'Training Accuracy', 'Test Accuracy']):\n",
    "        ax.set_xlabel('Epoch'); ax.set_ylabel(yl); ax.set_title(t); ax.legend(fontsize=9); ax.grid(True, alpha=0.3)\n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight'); print(f\"    Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_batch_size_comparison(histories_dict, title=\"Batch Size Comparison\", save_path=None):\n",
    "    \"\"\"Fig 5/6: Different batch size comparison\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(histories_dict)))\n",
    "    for idx, (bs, h) in enumerate(histories_dict.items()):\n",
    "        ep = range(1, len(h['train_loss']) + 1)\n",
    "        c = colors[idx]\n",
    "        axes[0].plot(ep, h['train_loss'], color=c, marker='o', ms=3, lw=1.5, label=f'BS={bs}')\n",
    "        axes[1].plot(ep, h['train_acc'], color=c, marker='s', ms=3, lw=1.5, label=f'BS={bs}')\n",
    "        axes[2].plot(ep, h['test_acc'], color=c, marker='^', ms=3, lw=1.5, label=f'BS={bs}')\n",
    "    for ax, yl, t in zip(axes, ['Loss', 'Accuracy', 'Accuracy'],\n",
    "                          ['Training Loss', 'Training Accuracy', 'Test Accuracy']):\n",
    "        ax.set_xlabel('Epoch'); ax.set_ylabel(yl); ax.set_title(t); ax.legend(fontsize=9); ax.grid(True, alpha=0.3)\n",
    "    fig.suptitle(title, fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight'); print(f\"    Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_predictions_table(texts, true_labels, pred_labels, n=100, save_path=None):\n",
    "    \"\"\"Fig 7: Prediction results table + grid\"\"\"\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    n = min(n, len(texts))\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8), gridspec_kw={'width_ratios': [1, 3]})\n",
    "    grid_size = int(np.ceil(np.sqrt(n)))\n",
    "    grid = np.zeros((grid_size, grid_size))\n",
    "    for i in range(n):\n",
    "        r, c = i // grid_size, i % grid_size\n",
    "        grid[r][c] = 1 if true_labels[i] == pred_labels[i] else -1\n",
    "    cmap = ListedColormap(['#ff6b6b', 'white', '#51cf66'])\n",
    "    axes[0].imshow(grid, cmap=cmap, vmin=-1, vmax=1, aspect='equal')\n",
    "    axes[0].set_title(f'Prediction Grid ({n})\\nGreen=Correct, Red=Wrong', fontsize=11)\n",
    "    axes[1].axis('off')\n",
    "    show_n = min(20, n)\n",
    "    rows, colors_list = [], []\n",
    "    label_map = {0: 'Fake', 1: 'Real'}\n",
    "    for i in range(show_n):\n",
    "        txt = texts[i][:55] + '...' if len(str(texts[i])) > 55 else str(texts[i])\n",
    "        ok = 'Yes' if true_labels[i] == pred_labels[i] else 'No'\n",
    "        rows.append([i+1, txt, label_map[int(true_labels[i])], label_map[int(pred_labels[i])], ok])\n",
    "        colors_list.append(['#d4edda' if ok == 'Yes' else '#f8d7da'] * 5)\n",
    "    t = axes[1].table(cellText=rows, colLabels=['#', 'Text', 'True', 'Pred', 'OK'],\n",
    "                      cellColours=colors_list, loc='upper center', cellLoc='left')\n",
    "    t.auto_set_font_size(False); t.set_fontsize(8); t.auto_set_column_width([0,1,2,3,4])\n",
    "    correct = sum(1 for i in range(n) if true_labels[i] == pred_labels[i])\n",
    "    fig.suptitle(f'Predictions: {correct}/{n} correct ({100*correct/n:.1f}%)', fontsize=14, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight'); print(f\"    Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(true_labels, pred_labels, save_path=None):\n",
    "    \"\"\"Fig 8: Confusion Matrix\"\"\"\n",
    "    cm = cm_func(true_labels, pred_labels)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Fake (0)', 'Real (1)'], yticklabels=['Fake (0)', 'Real (1)'],\n",
    "                ax=ax, annot_kws={\"size\": 16})\n",
    "    ax.set_xlabel('Predicted', fontsize=12); ax.set_ylabel('Actual', fontsize=12)\n",
    "    ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight'); print(f\"    Saved: {save_path}\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_auc(true_labels, pred_probs, save_path=None):\n",
    "    \"\"\"ROC-AUC Curve (from reference notebook)\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(true_labels, pred_probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    ax.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.4f})')\n",
    "    ax.plot([0, 1], [0, 1], 'navy', lw=2, linestyle='--')\n",
    "    ax.set_xlim([0, 1]); ax.set_ylim([0, 1.05])\n",
    "    ax.set_xlabel('False Positive Rate', fontsize=12)\n",
    "    ax.set_ylabel('True Positive Rate', fontsize=12)\n",
    "    ax.set_title('ROC Curve', fontsize=14, fontweight='bold')\n",
    "    ax.legend(loc='lower right', fontsize=12); ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight'); print(f\"    Saved: {save_path}\")\n",
    "    plt.show()\n",
    "    return roc_auc\n",
    "\n",
    "print(\"‚úì Visualization module defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 8: Chain-of-Thought Reasoning (BERT Attention)\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "CoT using DistilBERT's multi-head attention:\n",
    "- Extract attention from ALL layers and heads\n",
    "- Average across heads/layers ‚Üí word-level importance\n",
    "- Combine with rule-based feature analysis\n",
    "\"\"\"\n",
    "\n",
    "SENSATIONAL_WORDS = {\n",
    "    'shocking', 'unbelievable', 'breaking', 'exclusive', 'urgent',\n",
    "    'bombshell', 'horrifying', 'terrifying', 'scandal', 'outrage',\n",
    "    'devastating', 'explosive', 'stunning', 'alarming', 'incredible',\n",
    "    'exposed', 'secret', 'leaked', 'conspiracy', 'hoax', 'destroyed',\n",
    "    'slammed', 'blasted', 'ripped', 'epic', 'insane'\n",
    "}\n",
    "CREDIBILITY_PHRASES = [\n",
    "    'according to', 'research shows', 'study finds', 'officials said',\n",
    "    'reuters', 'associated press', 'confirmed by', 'evidence suggests',\n",
    "    'data shows', 'report says', 'investigation found', 'spokesperson said',\n",
    "    'published in', 'university of', 'official statement', 'government report'\n",
    "]\n",
    "EMOTIONAL_WORDS = {\n",
    "    'hate', 'love', 'angry', 'furious', 'amazing', 'terrible',\n",
    "    'disgusting', 'wonderful', 'horrible', 'fantastic', 'awful',\n",
    "    'outrageous', 'evil', 'hero', 'villain', 'miracle', 'disaster',\n",
    "    'tragic', 'brilliant', 'stupid', 'genius', 'idiot', 'corrupt'\n",
    "}\n",
    "CLICKBAIT_PATTERNS = [\n",
    "    'you won', 'believe', 'click here', 'share this', 'going viral',\n",
    "    'mind blowing', 'what happened next', 'will shock', 'doctors hate',\n",
    "    'one weird trick', 'exposed', 'they don want'\n",
    "]\n",
    "\n",
    "\n",
    "class BertChainOfThought:\n",
    "    \"\"\"Chain-of-Thought analyzer using DistilBERT attention weights\"\"\"\n",
    "\n",
    "    def __init__(self, model, tokenizer, device, max_len=256):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def analyze(self, text):\n",
    "        self.model.eval()\n",
    "        encoding = self.tokenizer(\n",
    "            text, max_length=self.max_len, padding='max_length',\n",
    "            truncation=True, return_tensors='pt')\n",
    "        ids = encoding['input_ids'].to(self.device)\n",
    "        mask = encoding['attention_mask'].to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits, attentions = self.model(ids, mask, return_attention=True)\n",
    "\n",
    "        prob = torch.sigmoid(logits).item()\n",
    "        prediction = 'Real' if prob > 0.5 else 'Fake'\n",
    "        confidence = prob if prob > 0.5 else 1 - prob\n",
    "\n",
    "        # Average attention: all layers, all heads ‚Üí [CLS] row\n",
    "        # attentions: tuple of (1, num_heads, seq_len, seq_len) for each layer\n",
    "        attn_all = torch.stack(attentions)  # (layers, 1, heads, seq, seq)\n",
    "        cls_attn = attn_all[:, 0, :, 0, :].mean(dim=(0, 1))  # avg over layers and heads ‚Üí (seq_len,)\n",
    "        cls_attn = cls_attn.cpu().numpy()\n",
    "\n",
    "        # Map back to tokens\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(ids[0].cpu())\n",
    "        real_len = mask[0].sum().item()\n",
    "        word_attn = {}\n",
    "        for i in range(1, real_len - 1):  # skip [CLS] and [SEP]\n",
    "            token = tokens[i]\n",
    "            if token.startswith('##'):\n",
    "                continue  # skip subword continuations\n",
    "            word_attn[token] = max(word_attn.get(token, 0), cls_attn[i])\n",
    "        top_words = sorted(word_attn.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "        features = self._analyze_features(text)\n",
    "        reasoning = self._build_reasoning(prediction, confidence, top_words, features)\n",
    "\n",
    "        return {\n",
    "            'text_preview': text[:300] + '...' if len(text) > 300 else text,\n",
    "            'prediction': prediction,\n",
    "            'confidence': f\"{confidence:.2%}\",\n",
    "            'probability': prob,\n",
    "            'top_attention_words': [(w, f\"{a:.4f}\") for w, a in top_words[:5]],\n",
    "            'features': features,\n",
    "            'reasoning': reasoning\n",
    "        }\n",
    "\n",
    "    def _analyze_features(self, text):\n",
    "        words = text.lower().split()\n",
    "        text_lower = text.lower()\n",
    "        sens = [w for w in words if w in SENSATIONAL_WORDS]\n",
    "        cred = [p for p in CREDIBILITY_PHRASES if p in text_lower]\n",
    "        emot = [w for w in words if w in EMOTIONAL_WORDS]\n",
    "        click = [p for p in CLICKBAIT_PATTERNS if p in text_lower]\n",
    "        def level(items, hi=2, lo=1):\n",
    "            return 'HIGH' if len(items) >= hi else 'MEDIUM' if len(items) >= lo else 'LOW'\n",
    "        return {\n",
    "            'sensational': (level(sens), sens[:5]),\n",
    "            'credibility': (level(cred), cred[:3]),\n",
    "            'emotional': (level(emot), emot[:5]),\n",
    "            'clickbait': (level(click), click[:3]),\n",
    "            'word_count': len(words)\n",
    "        }\n",
    "\n",
    "    def _build_reasoning(self, prediction, confidence, top_words, features):\n",
    "        lines = [\"=\" * 50, \"Step 1 - Text Feature Analysis:\"]\n",
    "        for key, (lvl, items) in [('sensational', features['sensational']),\n",
    "                                   ('credibility', features['credibility']),\n",
    "                                   ('emotional', features['emotional']),\n",
    "                                   ('clickbait', features['clickbait'])]:\n",
    "            lines.append(f\"  [{key.title():20s}] {lvl}\")\n",
    "            if items:\n",
    "                lines.append(f\"    Found: {', '.join(str(x) for x in items)}\")\n",
    "        lines.append(f\"  [{'Word Count':20s}] {features['word_count']}\")\n",
    "\n",
    "        lines.append(\"\\nStep 2 - DistilBERT Attention Key Tokens:\")\n",
    "        for token, weight in top_words[:5]:\n",
    "            bar = '‚ñà' * int(float(weight) * 200)\n",
    "            lines.append(f\"  '{token}' [{weight}] {bar}\")\n",
    "\n",
    "        lines.append(\"\\nStep 3 - Reasoning Chain:\")\n",
    "        reasons = []\n",
    "        if prediction == 'Fake':\n",
    "            if features['sensational'][0] != 'LOW':\n",
    "                reasons.append(\"Sensational/exaggerated language detected ‚Üí common in fabricated news.\")\n",
    "            if features['credibility'][0] == 'LOW':\n",
    "                reasons.append(\"No references to credible sources ‚Üí reduces reliability.\")\n",
    "            if features['emotional'][0] != 'LOW':\n",
    "                reasons.append(\"Highly emotional language ‚Üí often used to manipulate readers.\")\n",
    "            if features['clickbait'][0] != 'LOW':\n",
    "                reasons.append(\"Clickbait patterns present ‚Üí prioritizes engagement over accuracy.\")\n",
    "            if not reasons:\n",
    "                reasons.append(\"DistilBERT's learned contextual patterns match fake news characteristics.\")\n",
    "        else:\n",
    "            if features['credibility'][0] != 'LOW':\n",
    "                reasons.append(\"References to credible sources ‚Üí consistent with legitimate news.\")\n",
    "            if features['sensational'][0] == 'LOW':\n",
    "                reasons.append(\"Neutral, factual language ‚Üí consistent with professional journalism.\")\n",
    "            if features['emotional'][0] == 'LOW':\n",
    "                reasons.append(\"Objective tone ‚Üí no excessive emotional manipulation.\")\n",
    "            if not reasons:\n",
    "                reasons.append(\"DistilBERT's learned contextual patterns match real news characteristics.\")\n",
    "        for i, r in enumerate(reasons, 1):\n",
    "            lines.append(f\"  {i}. {r}\")\n",
    "\n",
    "        lines.extend([f\"\\n{'=' * 50}\",\n",
    "                      f\"Conclusion: [{prediction}] with {confidence:.2%} confidence.\",\n",
    "                      \"=\" * 50])\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "print(\"‚úì Chain-of-Thought module defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 9: Configuration & Experiment Helper\n",
    "# ============================================================\n",
    "import time\n",
    "\n",
    "CONFIG = {\n",
    "    'model_name': 'distilbert-base-uncased',\n",
    "    'max_len': 256,\n",
    "    'num_epochs': 4,       # BERT converges fast (2-4 epochs is standard)\n",
    "    'batch_size': 32,\n",
    "    'learning_rate': 2e-5, # Standard BERT fine-tuning LR\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'dropout': 0.2,\n",
    "    'freeze_bert': False,  # Set True for faster training (only train head)\n",
    "}\n",
    "\n",
    "\n",
    "def run_bert_experiment(train_ds, val_ds, test_ds,\n",
    "                        loss_fn='bce', learning_rate=None,\n",
    "                        batch_size=None, num_epochs=None,\n",
    "                        freeze_bert=None):\n",
    "    \"\"\"Run a single DistilBERT experiment and return history + model\"\"\"\n",
    "    lr = learning_rate or CONFIG['learning_rate']\n",
    "    bs = batch_size or CONFIG['batch_size']\n",
    "    ep = num_epochs or CONFIG['num_epochs']\n",
    "    freeze = freeze_bert if freeze_bert is not None else CONFIG['freeze_bert']\n",
    "\n",
    "    model = DistilBertClassifier(\n",
    "        model_name=CONFIG['model_name'],\n",
    "        dropout=CONFIG['dropout'],\n",
    "        freeze_bert=freeze\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    total, trainable = model.count_parameters()\n",
    "    print(f\"  Model: {total/1e6:.1f}M total, {trainable/1e6:.1f}M trainable\")\n",
    "\n",
    "    if loss_fn == 'bce':\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "    elif loss_fn == 'focal':\n",
    "        criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown loss: {loss_fn}\")\n",
    "    criterion = criterion.to(DEVICE)\n",
    "\n",
    "    history, best_model = train_bert_model(\n",
    "        model, train_ds, val_ds, test_ds, criterion, DEVICE,\n",
    "        num_epochs=ep, batch_size=bs, learning_rate=lr,\n",
    "        weight_decay=CONFIG['weight_decay'], warmup_ratio=CONFIG['warmup_ratio'])\n",
    "\n",
    "    return history, best_model\n",
    "\n",
    "\n",
    "print(\"‚úì Configuration ready\")\n",
    "print(f\"  Device: {DEVICE}\")\n",
    "print(f\"  Config: {CONFIG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run Experiments (Fig 1-6)\n",
    "\n",
    "DistilBERT converges in 2-4 epochs (vs 10-20 for LSTM). Each experiment takes ~5-10 min on T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 10: Experiment 1 ‚Äî BCE Loss (Default Config) ‚Üí Fig 1\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"[Exp 1] DistilBERT + BCE Loss (default config)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "t0 = time.time()\n",
    "history_bce, model_bce = run_bert_experiment(\n",
    "    train_dataset, val_dataset, test_dataset, loss_fn='bce')\n",
    "print(f\"  ‚úì Done in {(time.time()-t0)/60:.1f} min\")\n",
    "\n",
    "plot_training_curves(\n",
    "    history_bce,\n",
    "    title=\"Fig 1: DistilBERT + BCE Loss ‚Äî Default Config\",\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig1_bert_bce.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 11: Experiment 2 ‚Äî Focal Loss (Default Config) ‚Üí Fig 2\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"[Exp 2] DistilBERT + Focal Loss (default config)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "t0 = time.time()\n",
    "history_focal, model_focal = run_bert_experiment(\n",
    "    train_dataset, val_dataset, test_dataset, loss_fn='focal')\n",
    "print(f\"  ‚úì Done in {(time.time()-t0)/60:.1f} min\")\n",
    "\n",
    "plot_training_curves(\n",
    "    history_focal,\n",
    "    title=\"Fig 2: DistilBERT + Focal Loss ‚Äî Default Config\",\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig2_bert_focal.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 12: Experiment 3 ‚Äî Learning Rate Comparison ‚Üí Fig 3 & 4\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"[Exp 3] Learning Rate Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# BERT-specific LR range (much smaller than LSTM)\n",
    "learning_rates = [5e-5, 2e-5, 1e-5, 5e-6]\n",
    "\n",
    "# --- Fig 3: BCE + different LR ---\n",
    "print(\"\\n  === BCE Loss ===\")\n",
    "lr_histories_bce = {}\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n  LR = {lr}:\")\n",
    "    t0 = time.time()\n",
    "    h, _ = run_bert_experiment(train_dataset, val_dataset, test_dataset,\n",
    "                               loss_fn='bce', learning_rate=lr)\n",
    "    lr_histories_bce[lr] = h\n",
    "    print(f\"    ({(time.time()-t0)/60:.1f} min)\")\n",
    "\n",
    "plot_lr_comparison(\n",
    "    lr_histories_bce,\n",
    "    title=\"Fig 3: DistilBERT + BCE ‚Äî Learning Rate Comparison\",\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig3_bert_lr_bce.png\")\n",
    ")\n",
    "\n",
    "# --- Fig 4: Focal + different LR ---\n",
    "print(\"\\n  === Focal Loss ===\")\n",
    "lr_histories_focal = {}\n",
    "for lr in learning_rates:\n",
    "    print(f\"\\n  LR = {lr}:\")\n",
    "    t0 = time.time()\n",
    "    h, _ = run_bert_experiment(train_dataset, val_dataset, test_dataset,\n",
    "                               loss_fn='focal', learning_rate=lr)\n",
    "    lr_histories_focal[lr] = h\n",
    "    print(f\"    ({(time.time()-t0)/60:.1f} min)\")\n",
    "\n",
    "plot_lr_comparison(\n",
    "    lr_histories_focal,\n",
    "    title=\"Fig 4: DistilBERT + Focal ‚Äî Learning Rate Comparison\",\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig4_bert_lr_focal.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 13: Experiment 4 ‚Äî Batch Size Comparison ‚Üí Fig 5 & 6\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"[Exp 4] Batch Size Comparison\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "batch_sizes = [16, 32, 64]\n",
    "\n",
    "# --- Fig 5: BCE + different BS ---\n",
    "print(\"\\n  === BCE Loss ===\")\n",
    "bs_histories_bce = {}\n",
    "for bs in batch_sizes:\n",
    "    print(f\"\\n  Batch Size = {bs}:\")\n",
    "    t0 = time.time()\n",
    "    h, _ = run_bert_experiment(train_dataset, val_dataset, test_dataset,\n",
    "                               loss_fn='bce', batch_size=bs)\n",
    "    bs_histories_bce[bs] = h\n",
    "    print(f\"    ({(time.time()-t0)/60:.1f} min)\")\n",
    "\n",
    "plot_batch_size_comparison(\n",
    "    bs_histories_bce,\n",
    "    title=\"Fig 5: DistilBERT + BCE ‚Äî Batch Size Comparison\",\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig5_bert_bs_bce.png\")\n",
    ")\n",
    "\n",
    "# --- Fig 6: Focal + different BS ---\n",
    "print(\"\\n  === Focal Loss ===\")\n",
    "bs_histories_focal = {}\n",
    "for bs in batch_sizes:\n",
    "    print(f\"\\n  Batch Size = {bs}:\")\n",
    "    t0 = time.time()\n",
    "    h, _ = run_bert_experiment(train_dataset, val_dataset, test_dataset,\n",
    "                               loss_fn='focal', batch_size=bs)\n",
    "    bs_histories_focal[bs] = h\n",
    "    print(f\"    ({(time.time()-t0)/60:.1f} min)\")\n",
    "\n",
    "plot_batch_size_comparison(\n",
    "    bs_histories_focal,\n",
    "    title=\"Fig 6: DistilBERT + Focal ‚Äî Batch Size Comparison\",\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig6_bert_bs_focal.png\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Final Evaluation (Fig 7, 8 + ROC-AUC + Classification Report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 14: Final Evaluation ‚Üí Fig 7, 8, ROC-AUC, Classification Report\n",
    "# ============================================================\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"[Final Evaluation] Best DistilBERT + BCE Model\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Evaluate on test set\n",
    "criterion_eval = nn.BCEWithLogitsLoss().to(DEVICE)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
    "test_metrics, all_preds, all_labels, all_probs = evaluate_model(\n",
    "    model_bce, test_loader, criterion_eval, DEVICE)\n",
    "\n",
    "print(f\"\\n  ‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\")\n",
    "print(f\"  ‚ïë  Final Test Results (DistilBERT)     ‚ïë\")\n",
    "print(f\"  ‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\")\n",
    "print(f\"  ‚ïë  Accuracy:   {test_metrics['accuracy']:.4f}                 ‚ïë\")\n",
    "print(f\"  ‚ïë  Precision:  {test_metrics['precision']:.4f}                 ‚ïë\")\n",
    "print(f\"  ‚ïë  Recall:     {test_metrics['recall']:.4f}                 ‚ïë\")\n",
    "print(f\"  ‚ïë  F1 Score:   {test_metrics['f1']:.4f}                 ‚ïë\")\n",
    "print(f\"  ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\")\n",
    "\n",
    "# Detailed Classification Report (from reference notebook)\n",
    "print(\"\\n  --- Classification Report ---\")\n",
    "print(classification_report(all_labels, all_preds,\n",
    "                            target_names=['Fake News (0)', 'Real News (1)']))\n",
    "\n",
    "# Fig 7: Prediction Table\n",
    "plot_predictions_table(\n",
    "    list(X_test[:100]), all_labels[:100], all_preds[:100], n=100,\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig7_bert_predictions.png\")\n",
    ")\n",
    "\n",
    "# Fig 8: Confusion Matrix\n",
    "plot_confusion_matrix(\n",
    "    all_labels, all_preds,\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig8_bert_confusion.png\")\n",
    ")\n",
    "\n",
    "# ROC-AUC Curve (from reference notebook)\n",
    "roc_score = plot_roc_auc(\n",
    "    all_labels, all_probs,\n",
    "    save_path=os.path.join(OUTPUT_DIR, \"fig9_bert_roc_auc.png\")\n",
    ")\n",
    "print(f\"\\n  ROC-AUC Score: {roc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Chain-of-Thought Reasoning Demo\n",
    "\n",
    "Uses DistilBERT's multi-head self-attention (6 layers x 12 heads = 72 attention heads) to explain predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 15: Chain-of-Thought Reasoning Demo\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"[CoT] Chain-of-Thought Reasoning Demo\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "cot = BertChainOfThought(model_bce, tokenizer, DEVICE, MAX_LEN)\n",
    "\n",
    "# Analyze 5 test samples\n",
    "for i in range(5):\n",
    "    print(f\"\\n{'‚îÄ' * 60}\")\n",
    "    print(f\"  Sample {i+1}:\")\n",
    "    print(f\"{'‚îÄ' * 60}\")\n",
    "    result = cot.analyze(str(X_test[i]))\n",
    "    print(f\"\\nText: {result['text_preview']}\")\n",
    "    print(f\"\\nPrediction: {result['prediction']} (Confidence: {result['confidence']})\")\n",
    "    print(f\"Top Attention: {result['top_attention_words']}\")\n",
    "    print(f\"\\n{result['reasoning']}\")\n",
    "\n",
    "print(f\"\\n{'‚ïê' * 60}\")\n",
    "print(\"  ‚úì Chain-of-Thought analysis complete\")\n",
    "print(f\"{'‚ïê' * 60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Outputs & Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Cell 16: Save Outputs to Google Drive\n",
    "# ============================================================\n",
    "import shutil\n",
    "\n",
    "# Save model weights\n",
    "model_save_path = os.path.join(OUTPUT_DIR, \"best_distilbert_model.pt\")\n",
    "torch.save(model_bce.state_dict(), model_save_path)\n",
    "print(f\"  ‚úì Model saved: {model_save_path} ({os.path.getsize(model_save_path)/1e6:.1f} MB)\")\n",
    "\n",
    "# Copy everything to Google Drive\n",
    "drive_output = os.path.join(DATA_ROOT, 'outputs_bert')\n",
    "os.makedirs(drive_output, exist_ok=True)\n",
    "\n",
    "count = 0\n",
    "for fname in os.listdir(OUTPUT_DIR):\n",
    "    src = os.path.join(OUTPUT_DIR, fname)\n",
    "    dst = os.path.join(drive_output, fname)\n",
    "    shutil.copy2(src, dst)\n",
    "    print(f\"  ‚úì {fname}\")\n",
    "    count += 1\n",
    "\n",
    "print(f\"\\n  All {count} files saved to: {drive_output}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"  ‚úÖ All DistilBERT experiments completed!\")\n",
    "print(\"  Generated: fig1~fig9 + model weights\")\n",
    "print(\"  Model: DistilBERT (66M params)\")\n",
    "print(f\"  Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "print(f\"  ROC-AUC: {roc_score:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
